{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1157723-a934-4c81-a58a-8478c389c10d",
   "metadata": {},
   "source": [
    "**E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eeb7dc-31a9-4692-a37d-a385469db314",
   "metadata": {},
   "source": [
    "### Creating a dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d625694-f409-4275-8bff-31e9e7cbd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39382292-f5b8-4d82-86de-9054d4e8bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words2 = open('IN.txt', 'r').read().splitlines()\n",
    "words = words + words2\n",
    "words.remove('lllllllllllllllllll') # removing this word\n",
    "words = [word for word in words if re.match(\"^[a-zA-Z]+$\", word)] # removing all the words containing special characters\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb99c6ed-e081-4564-80a1-63de9a6bc26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94167c57-874b-4834-83c8-1ee42bd166e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38499"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "882be848-4cb2-430a-acfc-0a1e5390fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "458e0106-cc08-4a14-b4cb-6ef8e09df02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e -> m\n",
      "e m -> m\n",
      "m m -> a\n",
      "m a -> .\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "for w in words[:1]: \n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        print(ch1, ch2, '->', ch3)\n",
    "        xs.append((ix1, ix2))\n",
    "        ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf92e2f3-372f-4f30-b2c6-20143a18fa8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  5],\n",
       "         [ 5, 13],\n",
       "         [13, 13],\n",
       "         [13,  1]]),\n",
       " tensor([13, 13,  1,  0]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2663b716-4bf2-48ea-bfcb-429ceedfa599",
   "metadata": {},
   "source": [
    "### One hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67654a43-ae8e-402c-befc-f87cf1b09157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b266b012-8533-4aa1-bdd2-7802e45eb780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc = xenc.max(dim=1)[0]\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2fe623c1-39cb-4894-bd1f-5e6dbe62876e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 27])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6e6697a-c875-46a4-9e0a-8cd5434bf4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e564ca2a80>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104571f3-78d6-4918-b5ce-351b9e76078d",
   "metadata": {},
   "source": [
    "#### Weight initialization for 27 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae13c891-b66a-46cd-bf39-68b1274c332c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3012e+00,  1.7507e+00,  5.6962e-01,  1.8759e-01,  9.7085e-01,\n",
       "          8.2543e-01,  1.3337e+00, -1.3809e-01,  2.0006e+00, -1.1917e+00,\n",
       "          1.3436e+00,  8.0126e-01,  1.1799e+00,  6.3047e-01,  1.1622e+00,\n",
       "         -1.3341e+00,  2.3321e-01, -1.9661e+00,  4.5655e-01, -2.3403e-01,\n",
       "          7.5752e-01, -7.3300e-01,  1.8590e-01,  6.8520e-01, -2.3302e+00,\n",
       "         -4.8773e-01, -7.0939e-01],\n",
       "        [ 5.7711e-01, -1.4682e-01, -1.0297e+00, -5.5593e-02,  2.3374e-02,\n",
       "         -3.0376e-01,  1.2723e+00,  2.9948e-01,  2.7313e-01,  8.2372e-01,\n",
       "          4.6045e-01, -1.5518e+00, -5.4083e-01, -2.8017e-02,  1.5511e-01,\n",
       "         -1.0798e+00, -3.6466e-01, -7.7743e-02,  2.4754e+00, -4.8193e-01,\n",
       "         -1.8686e-01,  6.9767e-02,  3.2159e+00,  1.5740e+00,  3.5183e-01,\n",
       "          1.7567e-01, -5.5759e-01],\n",
       "        [-1.1710e+00,  6.6202e-01,  1.7278e+00,  1.0368e+00,  3.0283e-01,\n",
       "         -3.1877e-02, -3.3648e-01, -9.8572e-01,  5.6985e-01, -2.3764e+00,\n",
       "         -1.6078e-01,  2.6172e-01, -1.2037e+00,  2.1657e-01,  9.9301e-01,\n",
       "          2.9923e-01, -1.3196e+00, -1.6045e+00,  1.4091e+00, -4.0423e-01,\n",
       "         -3.2387e-01, -3.6911e-01, -1.6043e+00, -3.4211e-01,  2.0659e-01,\n",
       "          2.0420e+00,  3.5281e-01],\n",
       "        [-1.0187e+00,  7.6128e-02, -1.5147e+00, -4.7506e-01, -2.4315e+00,\n",
       "          2.6157e-02,  5.8193e-01,  8.6165e-01, -1.1987e+00, -1.2165e+00,\n",
       "         -2.2705e-02,  2.6525e-01,  1.4802e+00, -7.7012e-01,  1.2679e+00,\n",
       "          2.2100e-01, -1.6766e+00,  5.1876e-02,  1.3855e+00, -3.7149e-01,\n",
       "          2.2086e-01,  1.4995e+00, -7.7105e-01, -1.6217e+00,  6.6365e-01,\n",
       "         -1.3838e+00,  1.2080e+00],\n",
       "        [-2.6212e-01, -1.6884e+00, -5.7939e-01, -1.3291e-01, -9.8564e-01,\n",
       "          8.7240e-01,  1.3392e+00,  8.0565e-01,  6.1445e-01, -7.3227e-01,\n",
       "          9.8906e-01,  1.3044e+00, -7.3980e-01,  4.8360e-01,  2.0716e+00,\n",
       "         -1.3334e+00,  4.6059e-01,  8.0316e-01, -2.1209e+00,  1.3976e+00,\n",
       "          1.7772e+00, -1.9891e+00,  8.1795e-01,  1.1590e+00,  3.2126e-01,\n",
       "          8.0266e-01, -7.0714e-01],\n",
       "        [ 6.3525e-01, -1.1320e+00,  1.1322e+00,  1.5536e-02,  8.0482e-01,\n",
       "         -1.5626e+00,  1.3578e+00, -1.7322e+00,  4.0972e-01, -1.6451e+00,\n",
       "          4.9660e-01,  1.1995e+00,  8.7651e-01,  2.3734e-01, -9.9724e-01,\n",
       "         -4.6754e-01, -2.9867e-02, -1.2612e-01, -1.1501e+00, -3.4333e-02,\n",
       "         -8.6011e-01,  4.0953e-01,  1.8686e-02,  3.5685e-01,  8.2673e-01,\n",
       "         -2.7325e-01, -2.0036e-01],\n",
       "        [-1.0814e+00, -5.2867e-01,  7.4287e-01, -8.5577e-01, -1.5622e+00,\n",
       "         -7.4478e-01, -1.9256e-01, -9.4554e-01, -7.0757e-01, -6.4298e-01,\n",
       "          3.7875e-01, -8.9638e-01,  2.7515e+00,  6.3333e-01, -1.0926e-03,\n",
       "          5.4226e-02, -3.7137e-01,  8.7759e-01,  8.5978e-01, -1.2007e+00,\n",
       "          1.5456e+00,  5.9381e-01,  2.5324e-01, -7.3285e-02, -4.4751e-01,\n",
       "         -8.6807e-01,  6.9627e-01],\n",
       "        [-3.9857e-01, -5.9925e-01, -2.0663e+00, -6.2256e-01,  8.2881e-01,\n",
       "         -5.6439e-01, -4.5202e-01,  3.8420e-01,  1.5213e-01, -1.4851e-01,\n",
       "         -4.8893e-01,  1.7609e+00, -1.7585e+00, -1.0310e+00,  1.9426e+00,\n",
       "          7.6555e-02, -8.9451e-02,  5.8751e-01, -1.0811e+00, -1.5117e+00,\n",
       "         -7.3379e-02,  1.6684e+00,  1.0139e-01, -1.1501e+00,  8.4019e-01,\n",
       "         -1.1583e+00,  5.6804e-01],\n",
       "        [-1.5718e+00,  3.5462e-01, -1.8799e+00, -6.2638e-01, -2.8303e-01,\n",
       "          3.4266e-01,  9.1521e-01, -7.9337e-01, -3.9038e-01, -7.8342e-01,\n",
       "          2.5335e+00,  9.5982e-01,  3.9846e-01,  5.2557e-01, -1.0560e+00,\n",
       "          3.8793e-01,  2.1747e-01, -1.2554e+00, -7.1356e-01, -1.9431e-01,\n",
       "         -1.4084e+00, -1.3063e+00,  6.5696e-01, -1.4591e+00, -3.3216e-01,\n",
       "         -4.2810e-01,  2.2601e-01],\n",
       "        [ 3.2010e-01,  1.5249e-01,  1.0011e+00, -7.0487e-01,  1.7828e-01,\n",
       "          2.2389e-02, -2.3926e-01,  1.2558e+00, -1.0228e+00,  5.1320e-01,\n",
       "          6.5830e-01,  5.6376e-01,  1.5385e+00,  5.5980e-01,  8.2532e-01,\n",
       "          3.2400e+00, -1.6324e+00, -8.0303e-01, -2.4977e-01,  5.9984e-01,\n",
       "         -1.7351e-01,  1.2613e+00,  6.3328e-01,  6.6857e-01,  2.0307e+00,\n",
       "          1.1936e+00, -2.6207e-02],\n",
       "        [-1.0636e-01, -1.0461e+00, -6.6007e-01,  8.2035e-01, -6.5734e-01,\n",
       "         -1.4731e+00, -7.5206e-01, -2.5384e-01, -3.7800e-01,  1.2083e+00,\n",
       "         -1.6599e+00, -1.2745e+00,  1.9291e-01,  1.3293e-01, -1.7847e+00,\n",
       "         -5.0148e-02,  1.7867e+00, -6.7017e-01, -1.9482e+00, -2.0182e+00,\n",
       "         -2.6508e+00, -2.4045e-01, -1.4688e+00,  1.3452e+00, -1.3411e+00,\n",
       "          3.1739e-01, -4.8625e-01],\n",
       "        [-2.4304e+00, -6.0324e-01,  3.7195e-01,  1.2682e+00, -1.4309e+00,\n",
       "          1.7468e+00,  7.8955e-01, -8.8077e-01,  1.2885e+00, -1.6022e+00,\n",
       "         -2.6713e-01, -5.9820e-01, -1.0683e+00, -2.7061e-01, -2.4541e+00,\n",
       "         -2.0890e+00, -1.3761e+00,  1.2581e-01,  2.0295e+00,  2.1816e-01,\n",
       "          5.3230e-01, -9.5564e-01,  7.5585e-02,  5.4801e-01,  1.7394e-02,\n",
       "          9.4267e-02,  5.9185e-01],\n",
       "        [ 9.1446e-01, -1.7773e-01, -7.2704e-01, -1.7608e+00,  1.3338e+00,\n",
       "         -6.3828e-01, -2.0575e+00,  1.6321e-01, -3.4230e-01, -2.4927e-01,\n",
       "         -3.4404e-02,  1.4935e-02,  3.9510e-02, -5.5776e-01, -9.4607e-01,\n",
       "         -4.9422e-01, -2.3642e+00,  7.3268e-01, -2.1860e+00, -1.8382e+00,\n",
       "          1.3172e+00,  7.1197e-02, -6.9094e-01,  8.1765e-01, -2.6536e+00,\n",
       "         -8.2527e-01, -9.0494e-01],\n",
       "        [-2.2453e+00, -7.3282e-01, -8.0415e-01, -1.6725e+00, -2.2901e-01,\n",
       "         -1.8402e+00,  7.2010e-01, -5.1849e-01,  7.3577e-02, -3.7658e-01,\n",
       "          1.4168e-01,  3.2552e-01, -1.8883e-01,  3.6285e-01, -2.2261e-01,\n",
       "         -6.4888e-02,  1.0203e-01, -2.1066e+00,  5.7542e-01,  1.3716e+00,\n",
       "          3.8594e-01, -6.8212e-01,  1.4435e+00,  1.7339e+00,  8.7321e-01,\n",
       "         -1.3839e+00, -9.8577e-01],\n",
       "        [-8.9512e-01,  1.4559e+00, -1.2826e+00,  1.1723e+00, -4.2007e-01,\n",
       "          3.1104e-01, -1.2572e-01,  1.1712e+00, -2.5277e+00, -1.5158e+00,\n",
       "         -9.5366e-01, -8.8734e-01,  1.2782e+00,  6.8201e-01,  4.0220e-01,\n",
       "          9.0201e-01, -1.3881e+00,  6.4763e-01, -1.4141e+00,  2.0831e+00,\n",
       "          9.5174e-01, -2.3171e+00,  2.5601e-01, -5.4977e-01, -1.8346e+00,\n",
       "         -3.1633e-01,  1.3262e+00],\n",
       "        [ 8.3719e-01, -3.0867e-01, -1.1854e+00,  2.5927e-02,  3.7340e-01,\n",
       "          6.7027e-01, -4.1288e-04,  2.5011e+00,  9.2310e-01, -7.9874e-01,\n",
       "          3.0063e+00, -2.7521e-01,  9.7113e-01,  5.7447e-01,  2.4090e+00,\n",
       "          3.7366e-01, -1.2694e+00, -8.2880e-01, -1.4747e-01,  2.6333e+00,\n",
       "          8.9087e-01,  1.5907e+00,  4.5238e-01,  9.5361e-01, -8.0322e-01,\n",
       "         -2.3350e-02, -1.7809e+00],\n",
       "        [ 1.6310e+00, -4.8694e-01,  8.3023e-01,  1.0554e+00,  4.9596e-01,\n",
       "         -4.6440e-01, -1.0797e+00, -6.5706e-01, -5.9613e-01,  7.9859e-01,\n",
       "         -1.7723e+00, -4.7851e-01,  2.2887e-01,  2.5127e-01,  1.3179e+00,\n",
       "         -1.0624e+00, -1.8514e+00, -3.4258e-02, -8.7650e-01, -1.3728e+00,\n",
       "          3.0749e-01,  5.8182e-01, -5.2275e-02, -1.4701e-01,  1.0233e+00,\n",
       "         -1.5981e+00, -3.0194e-01],\n",
       "        [ 1.0844e-02, -1.0146e+00,  1.7101e-01, -7.0064e-01, -1.0897e-01,\n",
       "          3.8450e-01,  1.1096e+00, -4.6992e-01,  5.4676e-01, -7.1275e-01,\n",
       "          9.2332e-01, -1.3706e-01,  5.7440e-02, -3.0083e-01,  1.5755e+00,\n",
       "          8.9343e-02,  3.4884e-01,  3.5625e-01,  7.6716e-01, -1.3324e+00,\n",
       "         -4.4513e-01, -1.1374e+00, -7.9350e-01,  1.3360e+00, -9.4707e-02,\n",
       "         -6.0706e-02,  1.4690e+00],\n",
       "        [ 3.9252e-01,  2.4774e-01,  3.6545e-01,  6.4551e-01,  3.9143e-01,\n",
       "          6.5537e-01, -5.7396e-01, -7.1358e-01,  8.7486e-01,  3.6950e-01,\n",
       "         -8.9669e-01,  5.9399e-01,  1.0944e+00,  3.7055e-01,  3.4882e-01,\n",
       "         -6.0697e-01, -9.1252e-01,  2.2010e+00,  1.3944e+00, -3.0080e-01,\n",
       "          6.3289e-01, -3.9724e-01, -2.2540e-01, -9.3541e-01, -6.8927e-01,\n",
       "         -6.7798e-01, -1.1153e+00],\n",
       "        [-4.1807e-01,  4.4858e-01, -2.3755e-01,  1.1469e+00, -2.2511e-01,\n",
       "         -6.6929e-01,  8.8940e-01,  7.1633e-03,  1.2837e+00,  1.6369e+00,\n",
       "          1.3381e-01, -1.0789e+00,  3.4528e-01, -1.6211e+00,  1.3635e+00,\n",
       "         -6.7740e-01, -2.8014e-01, -5.3937e-01,  9.1254e-01,  4.6513e-01,\n",
       "         -9.5705e-01, -7.0418e-01,  1.3613e+00, -7.2306e-01,  1.6836e+00,\n",
       "         -5.8802e-02, -1.0441e+00],\n",
       "        [-8.9981e-01, -1.3970e+00,  1.4029e-01,  3.3535e-01, -7.3035e-02,\n",
       "          3.9778e-01,  6.9474e-01,  1.5429e+00, -2.3887e-01, -2.7288e-01,\n",
       "          1.0378e+00,  4.6827e-01,  2.4862e-01,  3.8119e-01, -1.6881e+00,\n",
       "         -7.2918e-01,  1.3273e+00, -8.5527e-01, -1.1010e+00, -1.3149e-01,\n",
       "         -1.2397e-02, -5.7841e-02, -1.9236e-01,  3.4523e-01,  1.1360e+00,\n",
       "         -1.9059e-01, -1.2167e-01],\n",
       "        [ 1.2248e+00,  7.9306e-01,  8.0437e-01,  8.7749e-01, -7.1335e-01,\n",
       "         -6.5609e-01,  1.2455e+00,  1.8872e-01, -8.3987e-01,  1.5655e-01,\n",
       "         -1.0382e+00, -1.4215e+00, -1.5177e+00,  1.6893e+00,  4.9551e-02,\n",
       "          8.3374e-01,  1.2253e+00, -1.8655e-01,  2.6671e-01, -1.9214e+00,\n",
       "         -1.9876e+00,  5.9855e-01, -1.3114e-01, -8.3714e-01,  6.7432e-02,\n",
       "          1.1744e-01,  7.5683e-01],\n",
       "        [-1.3276e+00, -5.0203e-01, -9.9662e-01,  1.4366e+00,  1.5579e-01,\n",
       "          9.8576e-01, -1.7859e+00,  2.2144e-01, -5.1152e-01,  1.8590e+00,\n",
       "         -8.4271e-01, -4.4022e-01,  3.0383e-01,  2.6604e-01,  1.4054e+00,\n",
       "         -3.9195e-01,  9.8681e-02,  3.1621e-01,  1.5363e+00,  5.7201e-01,\n",
       "         -3.5532e-01, -1.8162e+00,  7.5216e-01,  8.8721e-02,  1.6006e+00,\n",
       "          6.7249e-02,  3.9909e-01],\n",
       "        [ 7.8550e-01,  4.0425e-01,  1.8796e-01, -7.4315e-01,  1.3920e+00,\n",
       "         -2.5803e-01, -7.6652e-01,  4.3073e-01,  1.3407e+00, -8.5987e-01,\n",
       "          6.9994e-01,  1.3797e+00,  9.4946e-01,  1.4511e-01, -1.1974e+00,\n",
       "          2.9751e-01, -4.3841e-01, -1.4231e+00,  8.6340e-01,  4.4633e-01,\n",
       "          1.4916e+00,  3.5274e-01, -6.1788e-01, -1.4541e-01, -6.5266e-01,\n",
       "         -1.1365e+00,  7.4419e-01],\n",
       "        [ 1.3585e+00,  1.6559e+00, -1.4159e+00,  2.2607e-01, -1.1007e+00,\n",
       "         -5.2587e-01,  6.5846e-01, -1.2007e-01,  1.3723e+00,  3.0836e-01,\n",
       "         -1.4942e-01, -1.1541e+00,  4.5260e-01, -1.6663e-01,  5.7667e-01,\n",
       "         -8.4308e-01,  2.5909e-01,  1.7170e-01,  2.2193e-01,  2.3890e-01,\n",
       "          1.1973e+00, -2.1791e-01, -3.3144e-01,  9.0929e-01, -1.6523e-01,\n",
       "         -1.2737e-01, -7.6869e-01],\n",
       "        [ 1.7205e-01, -5.3387e-01, -2.7658e-01,  2.0027e+00, -9.4555e-01,\n",
       "         -2.5817e-01,  3.8852e-02, -1.4238e+00, -9.2769e-01,  1.2301e+00,\n",
       "          1.6934e-01, -5.8888e-01, -1.0697e+00,  1.2232e+00, -1.9382e-01,\n",
       "          1.3406e+00,  8.2490e-01, -1.2235e+00, -7.3506e-01, -3.5625e-01,\n",
       "          1.3338e-01,  6.0543e-02, -4.1404e-01,  3.3900e-01, -2.3845e+00,\n",
       "         -1.3094e-01,  6.0437e-01],\n",
       "        [-5.7734e-01,  1.3858e+00,  6.6368e-01, -1.2288e+00, -5.9717e-01,\n",
       "         -2.2287e-01,  1.0149e+00, -9.1857e-01, -2.0748e+00, -1.2500e+00,\n",
       "          8.3162e-01, -2.5538e+00, -1.7670e-01,  5.9821e-02, -5.6866e-01,\n",
       "          3.8258e-01, -4.5886e-02, -1.3043e+00, -1.1546e+00,  5.6084e-01,\n",
       "          5.9450e-01, -6.2848e-01,  2.1450e-01,  5.3864e-01,  8.8148e-01,\n",
       "          9.9549e-01, -2.3688e+00]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((27, 27))\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f1c1678-bee1-4474-97d0-bd5263d8a930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6659,  0.6186,  1.7018,  0.2031,  1.7757, -0.7372,  2.6915, -1.8703,\n",
       "          2.4103, -2.8367,  1.8402,  2.0008,  2.0564,  0.8678,  0.1649, -1.8017,\n",
       "          0.2033, -2.0923, -0.6935, -0.2684, -0.1026, -0.3235,  0.2046,  1.0421,\n",
       "         -1.5035, -0.7610, -0.9097],\n",
       "        [-1.6101, -1.8649,  0.3281, -1.6570,  0.5758, -3.4028,  2.0779, -2.2507,\n",
       "          0.4833, -2.0216,  0.6383,  1.5251,  0.6877,  0.6002, -1.2198, -0.5324,\n",
       "          0.0722, -2.2327, -0.5747,  1.3373, -0.4742, -0.2726,  1.4621,  2.0907,\n",
       "          1.6999, -1.6571, -1.1861],\n",
       "        [-2.2453, -0.7328, -0.8041, -1.6725, -0.2290, -1.8402,  0.7201, -0.5185,\n",
       "          0.0736, -0.3766,  0.1417,  0.3255, -0.1888,  0.3628, -0.2226, -0.0649,\n",
       "          0.1020, -2.1066,  0.5754,  1.3716,  0.3859, -0.6821,  1.4435,  1.7339,\n",
       "          0.8732, -1.3839, -0.9858],\n",
       "        [-1.6682, -0.8796, -1.8338, -1.7281, -0.2056, -2.1440,  1.9923, -0.2190,\n",
       "          0.3467,  0.4471,  0.6021, -1.2263, -0.7297,  0.3348, -0.0675, -1.1447,\n",
       "         -0.2626, -2.1843,  3.0509,  0.8897,  0.1991, -0.6123,  4.6594,  3.3079,\n",
       "          1.2250, -1.2082, -1.5434]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc @ W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05501666-8db7-43c2-91ad-641d0300f6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.1379e-01, 1.8564e+00, 5.4839e+00, 1.2252e+00, 5.9042e+00, 4.7847e-01,\n",
       "         1.4753e+01, 1.5407e-01, 1.1137e+01, 5.8616e-02, 6.2977e+00, 7.3950e+00,\n",
       "         7.8179e+00, 2.3817e+00, 1.1793e+00, 1.6503e-01, 1.2255e+00, 1.2341e-01,\n",
       "         4.9980e-01, 7.6463e-01, 9.0250e-01, 7.2364e-01, 1.2270e+00, 2.8350e+00,\n",
       "         2.2236e-01, 4.6721e-01, 4.0263e-01],\n",
       "        [1.9987e-01, 1.5492e-01, 1.3883e+00, 1.9071e-01, 1.7786e+00, 3.3280e-02,\n",
       "         7.9874e+00, 1.0532e-01, 1.6214e+00, 1.3244e-01, 1.8932e+00, 4.5954e+00,\n",
       "         1.9891e+00, 1.8225e+00, 2.9528e-01, 5.8718e-01, 1.0748e+00, 1.0724e-01,\n",
       "         5.6289e-01, 3.8087e+00, 6.2241e-01, 7.6141e-01, 4.3152e+00, 8.0906e+00,\n",
       "         5.4736e+00, 1.9068e-01, 3.0540e-01],\n",
       "        [1.0589e-01, 4.8055e-01, 4.4747e-01, 1.8777e-01, 7.9532e-01, 1.5879e-01,\n",
       "         2.0546e+00, 5.9542e-01, 1.0764e+00, 6.8620e-01, 1.1522e+00, 1.3847e+00,\n",
       "         8.2793e-01, 1.4374e+00, 8.0043e-01, 9.3717e-01, 1.1074e+00, 1.2166e-01,\n",
       "         1.7779e+00, 3.9418e+00, 1.4710e+00, 5.0555e-01, 4.2353e+00, 5.6624e+00,\n",
       "         2.3946e+00, 2.5060e-01, 3.7315e-01],\n",
       "        [1.8858e-01, 4.1493e-01, 1.5980e-01, 1.7762e-01, 8.1413e-01, 1.1719e-01,\n",
       "         7.3327e+00, 8.0331e-01, 1.4144e+00, 1.5638e+00, 1.8260e+00, 2.9339e-01,\n",
       "         4.8207e-01, 1.3977e+00, 9.3473e-01, 3.1833e-01, 7.6903e-01, 1.1256e-01,\n",
       "         2.1133e+01, 2.4344e+00, 1.2203e+00, 5.4208e-01, 1.0557e+02, 2.7326e+01,\n",
       "         3.4043e+00, 2.9873e-01, 2.1366e-01]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((xenc @ W).exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9247cb0-691b-4ddc-b5de-0230f06769aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0067, 0.0244, 0.0720, 0.0161, 0.0775, 0.0063, 0.1936, 0.0020, 0.1462,\n",
       "         0.0008, 0.0827, 0.0971, 0.1026, 0.0313, 0.0155, 0.0022, 0.0161, 0.0016,\n",
       "         0.0066, 0.0100, 0.0118, 0.0095, 0.0161, 0.0372, 0.0029, 0.0061, 0.0053],\n",
       "        [0.0040, 0.0031, 0.0277, 0.0038, 0.0355, 0.0007, 0.1595, 0.0021, 0.0324,\n",
       "         0.0026, 0.0378, 0.0917, 0.0397, 0.0364, 0.0059, 0.0117, 0.0215, 0.0021,\n",
       "         0.0112, 0.0760, 0.0124, 0.0152, 0.0862, 0.1615, 0.1093, 0.0038, 0.0061],\n",
       "        [0.0030, 0.0137, 0.0128, 0.0054, 0.0227, 0.0045, 0.0588, 0.0170, 0.0308,\n",
       "         0.0196, 0.0329, 0.0396, 0.0237, 0.0411, 0.0229, 0.0268, 0.0317, 0.0035,\n",
       "         0.0508, 0.1127, 0.0421, 0.0145, 0.1211, 0.1619, 0.0685, 0.0072, 0.0107],\n",
       "        [0.0010, 0.0023, 0.0009, 0.0010, 0.0045, 0.0006, 0.0405, 0.0044, 0.0078,\n",
       "         0.0086, 0.0101, 0.0016, 0.0027, 0.0077, 0.0052, 0.0018, 0.0042, 0.0006,\n",
       "         0.1166, 0.0134, 0.0067, 0.0030, 0.5824, 0.1508, 0.0188, 0.0016, 0.0012]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W # log-counts\n",
    "counts = logits.exp() # equivalent to N matrix.\n",
    "probs = counts / counts.sum(1, keepdims=True) # normalize to get the probabilities\n",
    "probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8ff1e9c2-0560-43de-b7e2-8e53f0ad3e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0516de09-e58a-4638-85c9-53c2b2d5ea70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5],\n",
       "        [ 5, 13],\n",
       "        [13, 13],\n",
       "        [13,  1]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SUMMARY ---------------------------->>\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81c65264-d4f8-48ef-89b4-f7d2bcf61fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13, 13,  1,  0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e8024bd-475a-41ac-96cd-ca743742f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons weights. each neuron recieves 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a5acd468-5954-4343-84d6-c3cb746eef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float().max(dim=1)[0] # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "# btw: the last 2 lines here are together called a 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8276ecf0-aaf1-4299-9170-68a28aeed709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 27])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43fc453-b202-4606-a2f2-b4d6bb8fce94",
   "metadata": {},
   "source": [
    "#### Loss Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "92861e99-9c1f-4aaf-800b-fbbc5d82ccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Trigram example 1: .e -> m (indexes tensor([0, 5]),13)\n",
      "input to the neural net: tensor([0, 5])\n",
      "output prob from the neural net: tensor([0.0553, 0.0250, 0.0096, 0.0069, 0.1052, 0.0111, 0.0008, 0.0244, 0.0042,\n",
      "        0.0296, 0.0174, 0.0200, 0.0033, 0.0047, 0.0170, 0.2354, 0.0551, 0.0004,\n",
      "        0.0717, 0.0037, 0.0517, 0.0103, 0.0045, 0.0017, 0.0044, 0.0104, 0.2165])\n",
      "label (actual next character): 13\n",
      "probability assigned to the net to the correct character: 0.004655060824006796\n",
      "log likelihood: -5.369800090789795\n",
      "negative log likelihood: 5.369800090789795\n",
      "-------\n",
      "Trigram example 2: em -> m (indexes tensor([ 5, 13]),13)\n",
      "input to the neural net: tensor([ 5, 13])\n",
      "output prob from the neural net: tensor([0.0216, 0.1402, 0.0287, 0.0414, 0.3203, 0.0138, 0.0059, 0.0200, 0.0284,\n",
      "        0.0118, 0.0125, 0.0431, 0.0036, 0.0115, 0.0038, 0.0085, 0.0406, 0.0013,\n",
      "        0.1423, 0.0030, 0.0241, 0.0056, 0.0159, 0.0013, 0.0339, 0.0024, 0.0143])\n",
      "label (actual next character): 13\n",
      "probability assigned to the net to the correct character: 0.011514964513480663\n",
      "log likelihood: -4.464107990264893\n",
      "negative log likelihood: 4.464107990264893\n",
      "-------\n",
      "Trigram example 3: mm -> a (indexes tensor([13, 13]),1)\n",
      "input to the neural net: tensor([13, 13])\n",
      "output prob from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 1\n",
      "probability assigned to the net to the correct character: 0.07367686182260513\n",
      "log likelihood: -2.6080665588378906\n",
      "negative log likelihood: 2.6080665588378906\n",
      "-------\n",
      "Trigram example 4: ma -> . (indexes tensor([13,  1]),0)\n",
      "input to the neural net: tensor([13,  1])\n",
      "output prob from the neural net: tensor([0.0125, 0.0170, 0.0514, 0.0089, 0.1092, 0.0165, 0.0764, 0.0088, 0.0410,\n",
      "        0.0021, 0.0205, 0.0181, 0.0346, 0.0080, 0.0091, 0.0062, 0.0639, 0.0025,\n",
      "        0.1563, 0.0083, 0.0257, 0.0065, 0.0018, 0.0179, 0.2106, 0.0630, 0.0031])\n",
      "label (actual next character): 0\n",
      "probability assigned to the net to the correct character: 0.012495790608227253\n",
      "log likelihood: -4.382363319396973\n",
      "negative log likelihood: 4.382363319396973\n",
      "=============\n",
      "average negative log likelihood, i.e. loss =  4.206084251403809\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(4)\n",
    "for i in range(4):\n",
    "    # i-th trigram:\n",
    "    x = xs[i] # input character index\n",
    "    y = ys[i].item() # label character index \n",
    "    print('-------')\n",
    "    print(f'Trigram example {i+1}: {itos[x[0].item()]}{itos[x[1].item()]} -> {itos[y]} (indexes {x},{y})')  \n",
    "    print('input to the neural net:', x)\n",
    "    print('output prob from the neural net:', probs[i])\n",
    "    print('label (actual next character):', y)\n",
    "    p = probs[i, y]\n",
    "    print('probability assigned to the net to the correct character:', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood:', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "print('=============')\n",
    "print('average negative log likelihood, i.e. loss = ', nlls.mean().item())\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb092d-98e0-4142-8a9e-b87639840c56",
   "metadata": {},
   "source": [
    "#### OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5825765d-db0c-4e77-b308-798cfc3cd8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5],\n",
       "        [ 5, 13],\n",
       "        [13, 13],\n",
       "        [13,  1]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0f25537c-640a-4a10-b0a0-b0faae983af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13, 13,  1,  0])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "729e748d-aaca-4d33-8be2-d04e5dc0597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron recieves 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e09380a4-9b0d-4a26-8588-4740359123e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.2061, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float().max(dim=1)[0] # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(4), ys].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bf66a3bb-f435-4ec2-8dd7-fa32aaea456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "W.grad = None \n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "65d70df8-f37e-4566-85c1-e61fd077daa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3814e-02,  6.2439e-03,  2.4014e-03,  1.7229e-03,  2.6312e-02,\n",
       "          2.7859e-03,  1.9837e-04,  6.0949e-03,  1.0440e-03,  7.3938e-03,\n",
       "          4.3535e-03,  4.9889e-03,  8.2009e-04, -2.4884e-01,  4.2515e-03,\n",
       "          5.8851e-02,  1.3767e-02,  8.8321e-05,  1.7922e-02,  9.2083e-04,\n",
       "          1.2927e-02,  2.5706e-03,  1.1346e-03,  4.1811e-04,  1.0916e-03,\n",
       "          2.6035e-03,  5.4117e-02],\n",
       "        [-2.4688e-01,  4.2585e-03,  1.2841e-02,  2.2276e-03,  2.7310e-02,\n",
       "          4.1294e-03,  1.9102e-02,  2.1880e-03,  1.0250e-02,  5.2745e-04,\n",
       "          5.1212e-03,  4.5357e-03,  8.6560e-03,  1.9911e-03,  2.2811e-03,\n",
       "          1.5540e-03,  1.5967e-02,  6.3022e-04,  3.9087e-02,  2.0691e-03,\n",
       "          6.4308e-03,  1.6218e-03,  4.5595e-04,  4.4713e-03,  5.2640e-02,\n",
       "          1.5757e-02,  7.7249e-04],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 1.9216e-02,  4.1301e-02,  9.5878e-03,  1.2078e-02,  1.0639e-01,\n",
       "          6.2432e-03,  1.6739e-03,  1.1083e-02,  8.1503e-03,  1.0345e-02,\n",
       "          7.4884e-03,  1.5764e-02,  1.7169e-03, -4.9596e-01,  5.2030e-03,\n",
       "          6.0967e-02,  2.3922e-02,  4.1562e-04,  5.3492e-02,  1.6675e-03,\n",
       "          1.8954e-02,  3.9774e-03,  5.1083e-03,  7.4474e-04,  9.5737e-03,\n",
       "          3.2073e-03,  5.7685e-02],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-2.3368e-01, -1.9227e-01,  3.2138e-02,  2.0897e-02,  1.2424e-01,\n",
       "          1.2598e-02,  2.7164e-02,  1.3408e-02,  4.7997e-02,  7.5818e-03,\n",
       "          1.0126e-02,  3.5041e-02,  1.2828e-02, -2.3846e-01,  6.9069e-03,\n",
       "          6.4795e-03,  4.0746e-02,  3.9894e-03,  9.0905e-02,  4.2712e-03,\n",
       "          1.7651e-02,  4.9909e-03,  7.7473e-03,  9.8850e-03,  9.1232e-02,\n",
       "          2.8074e-02,  7.5024e-03],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8f93e76c-9d0d-4a8f-837f-5ea8e0be901b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ca3f5ad8-549e-44ee-902d-06c7684c3b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ee97c6fb-ca96-4ff8-813a-27e92fa917a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1cf6d7ac-f987-4710-8d7d-bbc14b4b0a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1463, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float().max(dim=1)[0] # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(4), ys].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "885fc8ad-a339-4182-8962-c630c7a04f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "W.grad = None #set to zero the gradient\n",
    "loss.backward()\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "062a7632-99ad-4a19-962e-9718105023b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5585, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float().max(dim=1)[0] # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(4), ys].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f2ede-fe97-4987-b696-8653fdd5767e",
   "metadata": {},
   "source": [
    "Look loss is decreasing...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "fc69f496-8c37-4cfc-b34c-f833ef6c6b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# __________ !!! Optimization !!! yay, but this time actually _________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "810e8fe1-0eb3-46b8-8113-7e97220d0625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  474384\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "for w in words: \n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1, ix2))\n",
    "        ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647 + 1)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "468e06b4-d3a8-40d7-a90e-56ffe9f4dd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4496638774871826\n",
      "2.4496517181396484\n",
      "2.4496397972106934\n",
      "2.4496278762817383\n",
      "2.449615955352783\n",
      "2.449603796005249\n",
      "2.449592113494873\n",
      "2.449579954147339\n",
      "2.449568271636963\n",
      "2.449556350708008\n",
      "2.449544668197632\n",
      "2.4495327472686768\n",
      "2.4495208263397217\n",
      "2.4495091438293457\n",
      "2.449497699737549\n",
      "2.449486017227173\n",
      "2.449474573135376\n",
      "2.449462413787842\n",
      "2.449450969696045\n",
      "2.449439525604248\n",
      "2.449427843093872\n",
      "2.449416399002075\n",
      "2.4494049549102783\n",
      "2.4493935108184814\n",
      "2.4493823051452637\n",
      "2.4493706226348877\n",
      "2.44935941696167\n",
      "2.449347972869873\n",
      "2.4493367671966553\n",
      "2.4493253231048584\n",
      "2.4493143558502197\n",
      "2.4493026733398438\n",
      "2.449291706085205\n",
      "2.4492805004119873\n",
      "2.4492692947387695\n",
      "2.4492580890655518\n",
      "2.449247121810913\n",
      "2.4492359161376953\n",
      "2.4492249488830566\n",
      "2.449213981628418\n",
      "2.4492030143737793\n",
      "2.4491920471191406\n",
      "2.449180841445923\n",
      "2.4491701126098633\n",
      "2.4491593837738037\n",
      "2.449148178100586\n",
      "2.4491374492645264\n",
      "2.449126720428467\n",
      "2.4491162300109863\n",
      "2.4491050243377686\n",
      "2.449094533920288\n",
      "2.4490833282470703\n",
      "2.44907283782959\n",
      "2.4490621089935303\n",
      "2.44905161857605\n",
      "2.4490411281585693\n",
      "2.4490301609039307\n",
      "2.4490199089050293\n",
      "2.449009418487549\n",
      "2.4489989280700684\n",
      "2.448988437652588\n",
      "2.448977470397949\n",
      "2.448967218399048\n",
      "2.4489569664001465\n",
      "2.448946475982666\n",
      "2.4489362239837646\n",
      "2.4489259719848633\n",
      "2.4489152431488037\n",
      "2.4489049911499023\n",
      "2.448894739151001\n",
      "2.4488844871520996\n",
      "2.4488744735717773\n",
      "2.448863983154297\n",
      "2.4488539695739746\n",
      "2.4488437175750732\n",
      "2.448833465576172\n",
      "2.4488234519958496\n",
      "2.4488136768341064\n",
      "2.448803424835205\n",
      "2.4487931728363037\n",
      "2.4487833976745605\n",
      "2.448773145675659\n",
      "2.448763370513916\n",
      "2.4487533569335938\n",
      "2.4487433433532715\n",
      "2.448733329772949\n",
      "2.448723554611206\n",
      "2.448713541030884\n",
      "2.4487037658691406\n",
      "2.4486937522888184\n",
      "2.448683977127075\n",
      "2.448674201965332\n",
      "2.448664426803589\n",
      "2.4486546516418457\n",
      "2.4486448764801025\n",
      "2.4486348628997803\n",
      "2.4486258029937744\n",
      "2.4486160278320312\n",
      "2.448606252670288\n",
      "2.448596477508545\n",
      "2.448586940765381\n",
      "2.448577642440796\n",
      "2.448568105697632\n",
      "2.4485583305358887\n",
      "2.4485490322113037\n",
      "2.4485394954681396\n",
      "2.4485301971435547\n",
      "2.4485206604003906\n",
      "2.4485111236572266\n",
      "2.4485015869140625\n",
      "2.4484920501708984\n",
      "2.4484832286834717\n",
      "2.4484736919403076\n",
      "2.4484641551971436\n",
      "2.4484550952911377\n",
      "2.4484457969665527\n",
      "2.4484362602233887\n",
      "2.448427438735962\n",
      "2.448418140411377\n",
      "2.448408842086792\n",
      "2.448399543762207\n",
      "2.448390483856201\n",
      "2.448381185531616\n",
      "2.4483723640441895\n",
      "2.4483630657196045\n",
      "2.4483537673950195\n",
      "2.4483449459075928\n",
      "2.448336124420166\n",
      "2.448326587677002\n",
      "2.448317766189575\n",
      "2.4483087062835693\n",
      "2.4482998847961426\n",
      "2.448291063308716\n",
      "2.448281764984131\n",
      "2.448273181915283\n",
      "2.4482641220092773\n",
      "2.4482555389404297\n",
      "2.448246717453003\n",
      "2.448237657546997\n",
      "2.4482288360595703\n",
      "2.4482202529907227\n",
      "2.448211431503296\n",
      "2.448202610015869\n",
      "2.4481937885284424\n",
      "2.4481852054595947\n",
      "2.448176383972168\n",
      "2.448167562484741\n",
      "2.4481589794158936\n",
      "2.448150634765625\n",
      "2.4481418132781982\n",
      "2.4481329917907715\n",
      "2.448124885559082\n",
      "2.448115825653076\n",
      "2.4481074810028076\n",
      "2.448098659515381\n",
      "2.4480905532836914\n",
      "2.4480819702148438\n",
      "2.448073625564575\n",
      "2.4480648040771484\n",
      "2.448056697845459\n",
      "2.4480478763580322\n",
      "2.4480397701263428\n",
      "2.448031187057495\n",
      "2.4480230808258057\n",
      "2.448014736175537\n",
      "2.4480061531066895\n",
      "2.447998046875\n",
      "2.4479894638061523\n",
      "2.447981119155884\n",
      "2.4479730129241943\n",
      "2.447964906692505\n",
      "2.4479565620422363\n",
      "2.4479482173919678\n",
      "2.4479401111602783\n",
      "2.447932243347168\n",
      "2.4479241371154785\n",
      "2.44791579246521\n",
      "2.4479074478149414\n",
      "2.447899580001831\n",
      "2.4478914737701416\n",
      "2.447883129119873\n",
      "2.4478752613067627\n",
      "2.4478671550750732\n",
      "2.447859048843384\n",
      "2.4478509426116943\n",
      "2.447843313217163\n",
      "2.4478352069854736\n",
      "2.447827100753784\n",
      "2.447819232940674\n",
      "2.4478113651275635\n",
      "2.447803258895874\n",
      "2.4477956295013428\n",
      "2.4477875232696533\n",
      "2.447779893875122\n",
      "2.4477717876434326\n",
      "2.4477641582489014\n",
      "2.44775652885437\n",
      "2.4477486610412598\n",
      "2.4477405548095703\n",
      "2.447732925415039\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent\n",
    "for k in range(200):\n",
    "    xenc = F.one_hot(xs, num_classes=27).float().max(dim=1)[0] # input to the network: one-hot encoding\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(xs.shape[0]), ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    \n",
    "    loss.backward()\n",
    "    # update\n",
    "    \n",
    "    W.data += -6 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f039f907-d308-4f30-b987-21c9cdc442dd",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "418adc68-5f65-41f5-937c-1e8bc0398133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uunide.\n",
      "ianaaad.\n",
      "ulelay.\n",
      "ainna.\n",
      "ui.\n"
     ]
    }
   ],
   "source": [
    "# Finally, sample from the neural net model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):  # Generate 5 words\n",
    "    out = []\n",
    "    ix1, ix2 = 0, 0  # Start with two dots (start tokens)\n",
    "\n",
    "    while True:\n",
    "        # Create one-hot encoding of both indices and merge them\n",
    "        xenc = F.one_hot(torch.tensor([[ix1, ix2]]), num_classes=27).float().max(dim=1)[0]\n",
    "\n",
    "        # Predict log-counts\n",
    "        logits = xenc @ W\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "        # Sample the next character\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        out.append(itos[ix])  # Convert index to character\n",
    "\n",
    "        if ix == 0:  # Stop if end token is reached\n",
    "            break\n",
    "\n",
    "        # Shift indices: new pair is (previous ix2, new ix)\n",
    "        ix1, ix2 = ix2, ix\n",
    "\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1caa14-1178-42ca-b217-b979adebb4af",
   "metadata": {},
   "source": [
    "#### Some tweeks in the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "d0627167-9dc5-42eb-8bb4-89cca1d9ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Increase Model Capacity\n",
    "# Change W from (27,27) to (27*27, 27) to capture trigram relationships better\n",
    "g = torch.Generator().manual_seed(2147483647 + 1)\n",
    "W = torch.randn((27 * 27, 27), generator=g, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "daa5df23-ccec-422b-9bd9-df0252e59754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2990212440490723\n",
      "2.2985565662384033\n",
      "2.2980945110321045\n",
      "2.2976348400115967\n",
      "2.297178030014038\n",
      "2.2967238426208496\n",
      "2.296271800994873\n",
      "2.2958223819732666\n",
      "2.295375347137451\n",
      "2.2949306964874268\n",
      "2.2944886684417725\n",
      "2.294049024581909\n",
      "2.293611526489258\n",
      "2.2931764125823975\n",
      "2.2927439212799072\n",
      "2.292313575744629\n",
      "2.2918856143951416\n",
      "2.291459560394287\n",
      "2.2910358905792236\n",
      "2.2906150817871094\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "    xenc = F.one_hot(xs[:, 0] * 27 + xs[:, 1], num_classes=27*27).float()  # Encode as a single index\n",
    "    logits = xenc @ W  # Predict log-counts\n",
    "    counts = logits.exp()  # Counts, equivalent to N\n",
    "    probs = counts / (counts.sum(1, keepdims=True) + 1e-6) # for numerical stability\n",
    "    loss = -probs[torch.arange(xs.shape[0]), ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    W.data += -40 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "602a5301-3eb5-4d97-b865-af7725f812c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cari.\n",
      "daming.\n",
      "cwdms.\n",
      "vi.\n",
      "daraisccdcselen.\n"
     ]
    }
   ],
   "source": [
    " # Prediction using the trained model\n",
    "g = torch.Generator().manual_seed(2147483647+5)\n",
    "\n",
    "for i in range(5):  # Generate 5 words\n",
    "    out = []\n",
    "    ix1, ix2 = 0, 0  # Start with two dots (start tokens)\n",
    "\n",
    "    while True:\n",
    "        # Encode (ix1, ix2) into a single index\n",
    "        idx = ix1 * 27 + ix2\n",
    "        xenc = F.one_hot(torch.tensor([idx]), num_classes=27*27).float()\n",
    "\n",
    "        # Predict log-counts\n",
    "        logits = xenc @ W\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "        # Sample the next character\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        out.append(itos[ix])  # Convert index to character\n",
    "\n",
    "        if ix == 0:  # Stop if end token is reached\n",
    "            break\n",
    "\n",
    "        # Shift indices: new pair is (previous ix2, new ix)\n",
    "        ix1, ix2 = ix2, ix\n",
    "\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd281045-7f86-41c2-ac0e-6d9851ca4f23",
   "metadata": {},
   "source": [
    "**E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "dab103c8-298d-4365-a67d-e2e47ef8a495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  5],\n",
       "         [ 5, 13],\n",
       "         [13, 13],\n",
       "         ...,\n",
       "         [21, 22],\n",
       "         [22,  5],\n",
       "         [ 5,  2]]),\n",
       " tensor([13, 13,  1,  ...,  5,  2,  0]))"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "ca2b2b7b-56fd-4a46-8a07-76a24948c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "20a775c7-a438-472a-887e-15b26a56da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test, y_train, y_test = train_test_split(xs, ys, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "ae4e0497-7964-481c-bd36-bfed6fc21cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "27f0af06-4a12-4b70-b537-5d97bef7c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647 + 1)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "8abec848-84a7-42f6-8ae9-0f1954f11767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.442227602005005\n",
      "2.4422271251678467\n",
      "2.4422261714935303\n",
      "2.442225456237793\n",
      "2.4422247409820557\n",
      "2.4422240257263184\n",
      "2.442223310470581\n",
      "2.4422225952148438\n",
      "2.4422218799591064\n",
      "2.442221164703369\n",
      "2.442220449447632\n",
      "2.4422194957733154\n",
      "2.44221830368042\n",
      "2.4422178268432617\n",
      "2.4422173500061035\n",
      "2.442216396331787\n",
      "2.442215919494629\n",
      "2.4422149658203125\n",
      "2.442214250564575\n",
      "2.442213296890259\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent\n",
    "for k in range(20):\n",
    "    xenc = F.one_hot(X_train, num_classes=27).float().max(dim=1)[0] # input to the network: one-hot encoding\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(X_train.shape[0]), y_train].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    \n",
    "    loss.backward()\n",
    "    # update\n",
    "    \n",
    "    W.data += -15 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "0ab776ae-17ac-4a9e-85ff-664e2d12bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = 2.442213296890259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "a7b40a82-2ff3-4dce-95a3-9c59a9a196a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.44336199760437\n"
     ]
    }
   ],
   "source": [
    "xenc = F.one_hot(X_dev, num_classes=27).float().max(dim=1)[0] # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(X_dev.shape[0]), y_dev].log().mean()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "cceb0278-6f97-4fb8-a4af-152c92adde6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_error = 2.44336199760437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "a9332368-6e4e-4a34-ac38-8dd793acc865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4483754634857178\n"
     ]
    }
   ],
   "source": [
    "xenc = F.one_hot(X_test, num_classes=27).float().max(dim=1)[0] # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(X_test.shape[0]), y_test].log().mean()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "ac9e3fd9-02d2-4bfb-950c-6d53938bbed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error = 2.4483754634857178"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b67784-3bc5-4201-9aa6-7949d5752c37",
   "metadata": {},
   "source": [
    "**E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "6ab7b50a-74d6-478e-8173-e42e11cf82df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg: 0.0001, Dev Loss: 2.4593379497528076\n",
      "Reg: 0.001, Dev Loss: 2.546643018722534\n",
      "Reg: 0.01, Dev Loss: 2.8563008308410645\n",
      "Reg: 0.1, Dev Loss: 3.2115323543548584\n",
      "Reg: 1, Dev Loss: nan\n"
     ]
    }
   ],
   "source": [
    "reg_strengths = [0.0001, 0.001, 0.01, 0.1, 1] \n",
    "results = []\n",
    "for reg_lambda in reg_strengths:\n",
    "    W = torch.randn((27, 27), generator=g, requires_grad=True)  # Reinitialize W\n",
    "\n",
    "    for k in range(1000):  # Training loop\n",
    "        xenc = F.one_hot(X_train, num_classes=27).float().max(dim=1)[0]\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "        loss = -probs[torch.arange(X_train.shape[0]), y_train].log().mean() + reg_lambda * (W**2).sum()\n",
    "\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        W.data += -6 * W.grad\n",
    "\n",
    "    # Compute final loss on dev set\n",
    "    xenc_dev = F.one_hot(X_dev, num_classes=27).float().max(dim=1)[0]\n",
    "    logits_dev = xenc_dev @ W\n",
    "    counts_dev = logits_dev.exp()\n",
    "    probs_dev = counts_dev / counts_dev.sum(1, keepdims=True)\n",
    "    dev_loss = -probs_dev[torch.arange(X_dev.shape[0]), y_dev].log().mean().item()\n",
    "\n",
    "    results.append((reg_lambda, dev_loss))\n",
    "    print(f\"Reg: {reg_lambda}, Dev Loss: {dev_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227fd9e-c1e5-4232-a08f-eb92805699e8",
   "metadata": {},
   "source": [
    "Analyzing the Results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "5d6b4154-60d8-426b-aa33-6862ac7e0c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "reg_lambdas, dev_losses = zip(*results)\n",
    "plt.plot(reg_lambdas, dev_losses, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"Regularization Strength (λ)\")\n",
    "plt.ylabel(\"Dev Loss\")\n",
    "plt.title(\"Effect of Regularization on Dev Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "6062c70c-bbb4-41e7-88de-6ec0dc9effe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.0890, grad_fn=<AddBackward0>)\n",
      "tensor(3.9444, grad_fn=<AddBackward0>)\n",
      "tensor(3.8223, grad_fn=<AddBackward0>)\n",
      "tensor(3.7170, grad_fn=<AddBackward0>)\n",
      "tensor(3.6255, grad_fn=<AddBackward0>)\n",
      "tensor(3.5453, grad_fn=<AddBackward0>)\n",
      "tensor(3.4748, grad_fn=<AddBackward0>)\n",
      "tensor(3.4126, grad_fn=<AddBackward0>)\n",
      "tensor(3.3575, grad_fn=<AddBackward0>)\n",
      "tensor(3.3083, grad_fn=<AddBackward0>)\n",
      "tensor(3.2640, grad_fn=<AddBackward0>)\n",
      "tensor(3.2239, grad_fn=<AddBackward0>)\n",
      "tensor(3.1874, grad_fn=<AddBackward0>)\n",
      "tensor(3.1540, grad_fn=<AddBackward0>)\n",
      "tensor(3.1232, grad_fn=<AddBackward0>)\n",
      "tensor(3.0948, grad_fn=<AddBackward0>)\n",
      "tensor(3.0684, grad_fn=<AddBackward0>)\n",
      "tensor(3.0439, grad_fn=<AddBackward0>)\n",
      "tensor(3.0211, grad_fn=<AddBackward0>)\n",
      "tensor(2.9999, grad_fn=<AddBackward0>)\n",
      "tensor(2.9800, grad_fn=<AddBackward0>)\n",
      "tensor(2.9614, grad_fn=<AddBackward0>)\n",
      "tensor(2.9440, grad_fn=<AddBackward0>)\n",
      "tensor(2.9276, grad_fn=<AddBackward0>)\n",
      "tensor(2.9123, grad_fn=<AddBackward0>)\n",
      "tensor(2.8979, grad_fn=<AddBackward0>)\n",
      "tensor(2.8843, grad_fn=<AddBackward0>)\n",
      "tensor(2.8715, grad_fn=<AddBackward0>)\n",
      "tensor(2.8594, grad_fn=<AddBackward0>)\n",
      "tensor(2.8480, grad_fn=<AddBackward0>)\n",
      "tensor(2.8372, grad_fn=<AddBackward0>)\n",
      "tensor(2.8269, grad_fn=<AddBackward0>)\n",
      "tensor(2.8172, grad_fn=<AddBackward0>)\n",
      "tensor(2.8080, grad_fn=<AddBackward0>)\n",
      "tensor(2.7993, grad_fn=<AddBackward0>)\n",
      "tensor(2.7909, grad_fn=<AddBackward0>)\n",
      "tensor(2.7830, grad_fn=<AddBackward0>)\n",
      "tensor(2.7754, grad_fn=<AddBackward0>)\n",
      "tensor(2.7681, grad_fn=<AddBackward0>)\n",
      "tensor(2.7612, grad_fn=<AddBackward0>)\n",
      "tensor(2.7546, grad_fn=<AddBackward0>)\n",
      "tensor(2.7482, grad_fn=<AddBackward0>)\n",
      "tensor(2.7421, grad_fn=<AddBackward0>)\n",
      "tensor(2.7363, grad_fn=<AddBackward0>)\n",
      "tensor(2.7307, grad_fn=<AddBackward0>)\n",
      "tensor(2.7253, grad_fn=<AddBackward0>)\n",
      "tensor(2.7201, grad_fn=<AddBackward0>)\n",
      "tensor(2.7151, grad_fn=<AddBackward0>)\n",
      "tensor(2.7103, grad_fn=<AddBackward0>)\n",
      "tensor(2.7056, grad_fn=<AddBackward0>)\n",
      "tensor(2.7012, grad_fn=<AddBackward0>)\n",
      "tensor(2.6969, grad_fn=<AddBackward0>)\n",
      "tensor(2.6927, grad_fn=<AddBackward0>)\n",
      "tensor(2.6886, grad_fn=<AddBackward0>)\n",
      "tensor(2.6847, grad_fn=<AddBackward0>)\n",
      "tensor(2.6810, grad_fn=<AddBackward0>)\n",
      "tensor(2.6773, grad_fn=<AddBackward0>)\n",
      "tensor(2.6738, grad_fn=<AddBackward0>)\n",
      "tensor(2.6704, grad_fn=<AddBackward0>)\n",
      "tensor(2.6670, grad_fn=<AddBackward0>)\n",
      "tensor(2.6638, grad_fn=<AddBackward0>)\n",
      "tensor(2.6607, grad_fn=<AddBackward0>)\n",
      "tensor(2.6576, grad_fn=<AddBackward0>)\n",
      "tensor(2.6547, grad_fn=<AddBackward0>)\n",
      "tensor(2.6518, grad_fn=<AddBackward0>)\n",
      "tensor(2.6490, grad_fn=<AddBackward0>)\n",
      "tensor(2.6463, grad_fn=<AddBackward0>)\n",
      "tensor(2.6437, grad_fn=<AddBackward0>)\n",
      "tensor(2.6411, grad_fn=<AddBackward0>)\n",
      "tensor(2.6386, grad_fn=<AddBackward0>)\n",
      "tensor(2.6361, grad_fn=<AddBackward0>)\n",
      "tensor(2.6338, grad_fn=<AddBackward0>)\n",
      "tensor(2.6314, grad_fn=<AddBackward0>)\n",
      "tensor(2.6292, grad_fn=<AddBackward0>)\n",
      "tensor(2.6270, grad_fn=<AddBackward0>)\n",
      "tensor(2.6248, grad_fn=<AddBackward0>)\n",
      "tensor(2.6227, grad_fn=<AddBackward0>)\n",
      "tensor(2.6207, grad_fn=<AddBackward0>)\n",
      "tensor(2.6187, grad_fn=<AddBackward0>)\n",
      "tensor(2.6167, grad_fn=<AddBackward0>)\n",
      "tensor(2.6148, grad_fn=<AddBackward0>)\n",
      "tensor(2.6130, grad_fn=<AddBackward0>)\n",
      "tensor(2.6112, grad_fn=<AddBackward0>)\n",
      "tensor(2.6094, grad_fn=<AddBackward0>)\n",
      "tensor(2.6077, grad_fn=<AddBackward0>)\n",
      "tensor(2.6060, grad_fn=<AddBackward0>)\n",
      "tensor(2.6043, grad_fn=<AddBackward0>)\n",
      "tensor(2.6027, grad_fn=<AddBackward0>)\n",
      "tensor(2.6011, grad_fn=<AddBackward0>)\n",
      "tensor(2.5995, grad_fn=<AddBackward0>)\n",
      "tensor(2.5980, grad_fn=<AddBackward0>)\n",
      "tensor(2.5965, grad_fn=<AddBackward0>)\n",
      "tensor(2.5951, grad_fn=<AddBackward0>)\n",
      "tensor(2.5937, grad_fn=<AddBackward0>)\n",
      "tensor(2.5923, grad_fn=<AddBackward0>)\n",
      "tensor(2.5909, grad_fn=<AddBackward0>)\n",
      "tensor(2.5896, grad_fn=<AddBackward0>)\n",
      "tensor(2.5883, grad_fn=<AddBackward0>)\n",
      "tensor(2.5870, grad_fn=<AddBackward0>)\n",
      "tensor(2.5857, grad_fn=<AddBackward0>)\n",
      "tensor(2.5845, grad_fn=<AddBackward0>)\n",
      "tensor(2.5833, grad_fn=<AddBackward0>)\n",
      "tensor(2.5821, grad_fn=<AddBackward0>)\n",
      "tensor(2.5810, grad_fn=<AddBackward0>)\n",
      "tensor(2.5799, grad_fn=<AddBackward0>)\n",
      "tensor(2.5787, grad_fn=<AddBackward0>)\n",
      "tensor(2.5777, grad_fn=<AddBackward0>)\n",
      "tensor(2.5766, grad_fn=<AddBackward0>)\n",
      "tensor(2.5755, grad_fn=<AddBackward0>)\n",
      "tensor(2.5745, grad_fn=<AddBackward0>)\n",
      "tensor(2.5735, grad_fn=<AddBackward0>)\n",
      "tensor(2.5725, grad_fn=<AddBackward0>)\n",
      "tensor(2.5716, grad_fn=<AddBackward0>)\n",
      "tensor(2.5706, grad_fn=<AddBackward0>)\n",
      "tensor(2.5697, grad_fn=<AddBackward0>)\n",
      "tensor(2.5688, grad_fn=<AddBackward0>)\n",
      "tensor(2.5679, grad_fn=<AddBackward0>)\n",
      "tensor(2.5670, grad_fn=<AddBackward0>)\n",
      "tensor(2.5661, grad_fn=<AddBackward0>)\n",
      "tensor(2.5653, grad_fn=<AddBackward0>)\n",
      "tensor(2.5645, grad_fn=<AddBackward0>)\n",
      "tensor(2.5636, grad_fn=<AddBackward0>)\n",
      "tensor(2.5628, grad_fn=<AddBackward0>)\n",
      "tensor(2.5620, grad_fn=<AddBackward0>)\n",
      "tensor(2.5613, grad_fn=<AddBackward0>)\n",
      "tensor(2.5605, grad_fn=<AddBackward0>)\n",
      "tensor(2.5598, grad_fn=<AddBackward0>)\n",
      "tensor(2.5590, grad_fn=<AddBackward0>)\n",
      "tensor(2.5583, grad_fn=<AddBackward0>)\n",
      "tensor(2.5576, grad_fn=<AddBackward0>)\n",
      "tensor(2.5569, grad_fn=<AddBackward0>)\n",
      "tensor(2.5562, grad_fn=<AddBackward0>)\n",
      "tensor(2.5556, grad_fn=<AddBackward0>)\n",
      "tensor(2.5549, grad_fn=<AddBackward0>)\n",
      "tensor(2.5542, grad_fn=<AddBackward0>)\n",
      "tensor(2.5536, grad_fn=<AddBackward0>)\n",
      "tensor(2.5530, grad_fn=<AddBackward0>)\n",
      "tensor(2.5524, grad_fn=<AddBackward0>)\n",
      "tensor(2.5517, grad_fn=<AddBackward0>)\n",
      "tensor(2.5511, grad_fn=<AddBackward0>)\n",
      "tensor(2.5506, grad_fn=<AddBackward0>)\n",
      "tensor(2.5500, grad_fn=<AddBackward0>)\n",
      "tensor(2.5494, grad_fn=<AddBackward0>)\n",
      "tensor(2.5488, grad_fn=<AddBackward0>)\n",
      "tensor(2.5483, grad_fn=<AddBackward0>)\n",
      "tensor(2.5477, grad_fn=<AddBackward0>)\n",
      "tensor(2.5472, grad_fn=<AddBackward0>)\n",
      "tensor(2.5467, grad_fn=<AddBackward0>)\n",
      "tensor(2.5462, grad_fn=<AddBackward0>)\n",
      "tensor(2.5457, grad_fn=<AddBackward0>)\n",
      "tensor(2.5452, grad_fn=<AddBackward0>)\n",
      "tensor(2.5447, grad_fn=<AddBackward0>)\n",
      "tensor(2.5442, grad_fn=<AddBackward0>)\n",
      "tensor(2.5437, grad_fn=<AddBackward0>)\n",
      "tensor(2.5432, grad_fn=<AddBackward0>)\n",
      "tensor(2.5427, grad_fn=<AddBackward0>)\n",
      "tensor(2.5423, grad_fn=<AddBackward0>)\n",
      "tensor(2.5418, grad_fn=<AddBackward0>)\n",
      "tensor(2.5414, grad_fn=<AddBackward0>)\n",
      "tensor(2.5409, grad_fn=<AddBackward0>)\n",
      "tensor(2.5405, grad_fn=<AddBackward0>)\n",
      "tensor(2.5401, grad_fn=<AddBackward0>)\n",
      "tensor(2.5397, grad_fn=<AddBackward0>)\n",
      "tensor(2.5392, grad_fn=<AddBackward0>)\n",
      "tensor(2.5388, grad_fn=<AddBackward0>)\n",
      "tensor(2.5384, grad_fn=<AddBackward0>)\n",
      "tensor(2.5380, grad_fn=<AddBackward0>)\n",
      "tensor(2.5376, grad_fn=<AddBackward0>)\n",
      "tensor(2.5373, grad_fn=<AddBackward0>)\n",
      "tensor(2.5369, grad_fn=<AddBackward0>)\n",
      "tensor(2.5365, grad_fn=<AddBackward0>)\n",
      "tensor(2.5361, grad_fn=<AddBackward0>)\n",
      "tensor(2.5358, grad_fn=<AddBackward0>)\n",
      "tensor(2.5354, grad_fn=<AddBackward0>)\n",
      "tensor(2.5350, grad_fn=<AddBackward0>)\n",
      "tensor(2.5347, grad_fn=<AddBackward0>)\n",
      "tensor(2.5343, grad_fn=<AddBackward0>)\n",
      "tensor(2.5340, grad_fn=<AddBackward0>)\n",
      "tensor(2.5337, grad_fn=<AddBackward0>)\n",
      "tensor(2.5333, grad_fn=<AddBackward0>)\n",
      "tensor(2.5330, grad_fn=<AddBackward0>)\n",
      "tensor(2.5327, grad_fn=<AddBackward0>)\n",
      "tensor(2.5324, grad_fn=<AddBackward0>)\n",
      "tensor(2.5320, grad_fn=<AddBackward0>)\n",
      "tensor(2.5317, grad_fn=<AddBackward0>)\n",
      "tensor(2.5314, grad_fn=<AddBackward0>)\n",
      "tensor(2.5311, grad_fn=<AddBackward0>)\n",
      "tensor(2.5308, grad_fn=<AddBackward0>)\n",
      "tensor(2.5305, grad_fn=<AddBackward0>)\n",
      "tensor(2.5302, grad_fn=<AddBackward0>)\n",
      "tensor(2.5299, grad_fn=<AddBackward0>)\n",
      "tensor(2.5296, grad_fn=<AddBackward0>)\n",
      "tensor(2.5294, grad_fn=<AddBackward0>)\n",
      "tensor(2.5291, grad_fn=<AddBackward0>)\n",
      "tensor(2.5288, grad_fn=<AddBackward0>)\n",
      "tensor(2.5285, grad_fn=<AddBackward0>)\n",
      "tensor(2.5283, grad_fn=<AddBackward0>)\n",
      "tensor(2.5280, grad_fn=<AddBackward0>)\n",
      "tensor(2.5277, grad_fn=<AddBackward0>)\n",
      "tensor(2.5275, grad_fn=<AddBackward0>)\n",
      "tensor(2.5272, grad_fn=<AddBackward0>)\n",
      "tensor(2.5270, grad_fn=<AddBackward0>)\n",
      "tensor(2.5267, grad_fn=<AddBackward0>)\n",
      "tensor(2.5265, grad_fn=<AddBackward0>)\n",
      "tensor(2.5262, grad_fn=<AddBackward0>)\n",
      "tensor(2.5260, grad_fn=<AddBackward0>)\n",
      "tensor(2.5257, grad_fn=<AddBackward0>)\n",
      "tensor(2.5255, grad_fn=<AddBackward0>)\n",
      "tensor(2.5253, grad_fn=<AddBackward0>)\n",
      "tensor(2.5250, grad_fn=<AddBackward0>)\n",
      "tensor(2.5248, grad_fn=<AddBackward0>)\n",
      "tensor(2.5246, grad_fn=<AddBackward0>)\n",
      "tensor(2.5243, grad_fn=<AddBackward0>)\n",
      "tensor(2.5241, grad_fn=<AddBackward0>)\n",
      "tensor(2.5239, grad_fn=<AddBackward0>)\n",
      "tensor(2.5237, grad_fn=<AddBackward0>)\n",
      "tensor(2.5235, grad_fn=<AddBackward0>)\n",
      "tensor(2.5233, grad_fn=<AddBackward0>)\n",
      "tensor(2.5231, grad_fn=<AddBackward0>)\n",
      "tensor(2.5228, grad_fn=<AddBackward0>)\n",
      "tensor(2.5226, grad_fn=<AddBackward0>)\n",
      "tensor(2.5224, grad_fn=<AddBackward0>)\n",
      "tensor(2.5222, grad_fn=<AddBackward0>)\n",
      "tensor(2.5220, grad_fn=<AddBackward0>)\n",
      "tensor(2.5218, grad_fn=<AddBackward0>)\n",
      "tensor(2.5216, grad_fn=<AddBackward0>)\n",
      "tensor(2.5215, grad_fn=<AddBackward0>)\n",
      "tensor(2.5213, grad_fn=<AddBackward0>)\n",
      "tensor(2.5211, grad_fn=<AddBackward0>)\n",
      "tensor(2.5209, grad_fn=<AddBackward0>)\n",
      "tensor(2.5207, grad_fn=<AddBackward0>)\n",
      "tensor(2.5205, grad_fn=<AddBackward0>)\n",
      "tensor(2.5203, grad_fn=<AddBackward0>)\n",
      "tensor(2.5202, grad_fn=<AddBackward0>)\n",
      "tensor(2.5200, grad_fn=<AddBackward0>)\n",
      "tensor(2.5198, grad_fn=<AddBackward0>)\n",
      "tensor(2.5196, grad_fn=<AddBackward0>)\n",
      "tensor(2.5195, grad_fn=<AddBackward0>)\n",
      "tensor(2.5193, grad_fn=<AddBackward0>)\n",
      "tensor(2.5191, grad_fn=<AddBackward0>)\n",
      "tensor(2.5190, grad_fn=<AddBackward0>)\n",
      "tensor(2.5188, grad_fn=<AddBackward0>)\n",
      "tensor(2.5186, grad_fn=<AddBackward0>)\n",
      "tensor(2.5185, grad_fn=<AddBackward0>)\n",
      "tensor(2.5183, grad_fn=<AddBackward0>)\n",
      "tensor(2.5182, grad_fn=<AddBackward0>)\n",
      "tensor(2.5180, grad_fn=<AddBackward0>)\n",
      "tensor(2.5179, grad_fn=<AddBackward0>)\n",
      "tensor(2.5177, grad_fn=<AddBackward0>)\n",
      "tensor(2.5175, grad_fn=<AddBackward0>)\n",
      "tensor(2.5174, grad_fn=<AddBackward0>)\n",
      "tensor(2.5172, grad_fn=<AddBackward0>)\n",
      "tensor(2.5171, grad_fn=<AddBackward0>)\n",
      "tensor(2.5170, grad_fn=<AddBackward0>)\n",
      "tensor(2.5168, grad_fn=<AddBackward0>)\n",
      "tensor(2.5167, grad_fn=<AddBackward0>)\n",
      "tensor(2.5165, grad_fn=<AddBackward0>)\n",
      "tensor(2.5164, grad_fn=<AddBackward0>)\n",
      "tensor(2.5162, grad_fn=<AddBackward0>)\n",
      "tensor(2.5161, grad_fn=<AddBackward0>)\n",
      "tensor(2.5160, grad_fn=<AddBackward0>)\n",
      "tensor(2.5158, grad_fn=<AddBackward0>)\n",
      "tensor(2.5157, grad_fn=<AddBackward0>)\n",
      "tensor(2.5156, grad_fn=<AddBackward0>)\n",
      "tensor(2.5154, grad_fn=<AddBackward0>)\n",
      "tensor(2.5153, grad_fn=<AddBackward0>)\n",
      "tensor(2.5152, grad_fn=<AddBackward0>)\n",
      "tensor(2.5151, grad_fn=<AddBackward0>)\n",
      "tensor(2.5149, grad_fn=<AddBackward0>)\n",
      "tensor(2.5148, grad_fn=<AddBackward0>)\n",
      "tensor(2.5147, grad_fn=<AddBackward0>)\n",
      "tensor(2.5146, grad_fn=<AddBackward0>)\n",
      "tensor(2.5144, grad_fn=<AddBackward0>)\n",
      "tensor(2.5143, grad_fn=<AddBackward0>)\n",
      "tensor(2.5142, grad_fn=<AddBackward0>)\n",
      "tensor(2.5141, grad_fn=<AddBackward0>)\n",
      "tensor(2.5140, grad_fn=<AddBackward0>)\n",
      "tensor(2.5138, grad_fn=<AddBackward0>)\n",
      "tensor(2.5137, grad_fn=<AddBackward0>)\n",
      "tensor(2.5136, grad_fn=<AddBackward0>)\n",
      "tensor(2.5135, grad_fn=<AddBackward0>)\n",
      "tensor(2.5134, grad_fn=<AddBackward0>)\n",
      "tensor(2.5133, grad_fn=<AddBackward0>)\n",
      "tensor(2.5132, grad_fn=<AddBackward0>)\n",
      "tensor(2.5130, grad_fn=<AddBackward0>)\n",
      "tensor(2.5129, grad_fn=<AddBackward0>)\n",
      "tensor(2.5128, grad_fn=<AddBackward0>)\n",
      "tensor(2.5127, grad_fn=<AddBackward0>)\n",
      "tensor(2.5126, grad_fn=<AddBackward0>)\n",
      "tensor(2.5125, grad_fn=<AddBackward0>)\n",
      "tensor(2.5124, grad_fn=<AddBackward0>)\n",
      "tensor(2.5123, grad_fn=<AddBackward0>)\n",
      "tensor(2.5122, grad_fn=<AddBackward0>)\n",
      "tensor(2.5121, grad_fn=<AddBackward0>)\n",
      "tensor(2.5120, grad_fn=<AddBackward0>)\n",
      "tensor(2.5119, grad_fn=<AddBackward0>)\n",
      "tensor(2.5118, grad_fn=<AddBackward0>)\n",
      "tensor(2.5117, grad_fn=<AddBackward0>)\n",
      "tensor(2.5116, grad_fn=<AddBackward0>)\n",
      "tensor(2.5115, grad_fn=<AddBackward0>)\n",
      "tensor(2.5114, grad_fn=<AddBackward0>)\n",
      "tensor(2.5113, grad_fn=<AddBackward0>)\n",
      "tensor(2.5112, grad_fn=<AddBackward0>)\n",
      "tensor(2.5111, grad_fn=<AddBackward0>)\n",
      "tensor(2.5110, grad_fn=<AddBackward0>)\n",
      "tensor(2.5110, grad_fn=<AddBackward0>)\n",
      "tensor(2.5109, grad_fn=<AddBackward0>)\n",
      "tensor(2.5108, grad_fn=<AddBackward0>)\n",
      "tensor(2.5107, grad_fn=<AddBackward0>)\n",
      "tensor(2.5106, grad_fn=<AddBackward0>)\n",
      "tensor(2.5105, grad_fn=<AddBackward0>)\n",
      "tensor(2.5104, grad_fn=<AddBackward0>)\n",
      "tensor(2.5103, grad_fn=<AddBackward0>)\n",
      "tensor(2.5102, grad_fn=<AddBackward0>)\n",
      "tensor(2.5102, grad_fn=<AddBackward0>)\n",
      "tensor(2.5101, grad_fn=<AddBackward0>)\n",
      "tensor(2.5100, grad_fn=<AddBackward0>)\n",
      "tensor(2.5099, grad_fn=<AddBackward0>)\n",
      "tensor(2.5098, grad_fn=<AddBackward0>)\n",
      "tensor(2.5097, grad_fn=<AddBackward0>)\n",
      "tensor(2.5097, grad_fn=<AddBackward0>)\n",
      "tensor(2.5096, grad_fn=<AddBackward0>)\n",
      "tensor(2.5095, grad_fn=<AddBackward0>)\n",
      "tensor(2.5094, grad_fn=<AddBackward0>)\n",
      "tensor(2.5093, grad_fn=<AddBackward0>)\n",
      "tensor(2.5093, grad_fn=<AddBackward0>)\n",
      "tensor(2.5092, grad_fn=<AddBackward0>)\n",
      "tensor(2.5091, grad_fn=<AddBackward0>)\n",
      "tensor(2.5090, grad_fn=<AddBackward0>)\n",
      "tensor(2.5090, grad_fn=<AddBackward0>)\n",
      "tensor(2.5089, grad_fn=<AddBackward0>)\n",
      "tensor(2.5088, grad_fn=<AddBackward0>)\n",
      "tensor(2.5087, grad_fn=<AddBackward0>)\n",
      "tensor(2.5087, grad_fn=<AddBackward0>)\n",
      "tensor(2.5086, grad_fn=<AddBackward0>)\n",
      "tensor(2.5085, grad_fn=<AddBackward0>)\n",
      "tensor(2.5084, grad_fn=<AddBackward0>)\n",
      "tensor(2.5084, grad_fn=<AddBackward0>)\n",
      "tensor(2.5083, grad_fn=<AddBackward0>)\n",
      "tensor(2.5082, grad_fn=<AddBackward0>)\n",
      "tensor(2.5082, grad_fn=<AddBackward0>)\n",
      "tensor(2.5081, grad_fn=<AddBackward0>)\n",
      "tensor(2.5080, grad_fn=<AddBackward0>)\n",
      "tensor(2.5080, grad_fn=<AddBackward0>)\n",
      "tensor(2.5079, grad_fn=<AddBackward0>)\n",
      "tensor(2.5078, grad_fn=<AddBackward0>)\n",
      "tensor(2.5078, grad_fn=<AddBackward0>)\n",
      "tensor(2.5077, grad_fn=<AddBackward0>)\n",
      "tensor(2.5076, grad_fn=<AddBackward0>)\n",
      "tensor(2.5076, grad_fn=<AddBackward0>)\n",
      "tensor(2.5075, grad_fn=<AddBackward0>)\n",
      "tensor(2.5074, grad_fn=<AddBackward0>)\n",
      "tensor(2.5074, grad_fn=<AddBackward0>)\n",
      "tensor(2.5073, grad_fn=<AddBackward0>)\n",
      "tensor(2.5073, grad_fn=<AddBackward0>)\n",
      "tensor(2.5072, grad_fn=<AddBackward0>)\n",
      "tensor(2.5071, grad_fn=<AddBackward0>)\n",
      "tensor(2.5071, grad_fn=<AddBackward0>)\n",
      "tensor(2.5070, grad_fn=<AddBackward0>)\n",
      "tensor(2.5069, grad_fn=<AddBackward0>)\n",
      "tensor(2.5069, grad_fn=<AddBackward0>)\n",
      "tensor(2.5068, grad_fn=<AddBackward0>)\n",
      "tensor(2.5068, grad_fn=<AddBackward0>)\n",
      "tensor(2.5067, grad_fn=<AddBackward0>)\n",
      "tensor(2.5067, grad_fn=<AddBackward0>)\n",
      "tensor(2.5066, grad_fn=<AddBackward0>)\n",
      "tensor(2.5065, grad_fn=<AddBackward0>)\n",
      "tensor(2.5065, grad_fn=<AddBackward0>)\n",
      "tensor(2.5064, grad_fn=<AddBackward0>)\n",
      "tensor(2.5064, grad_fn=<AddBackward0>)\n",
      "tensor(2.5063, grad_fn=<AddBackward0>)\n",
      "tensor(2.5063, grad_fn=<AddBackward0>)\n",
      "tensor(2.5062, grad_fn=<AddBackward0>)\n",
      "tensor(2.5061, grad_fn=<AddBackward0>)\n",
      "tensor(2.5061, grad_fn=<AddBackward0>)\n",
      "tensor(2.5060, grad_fn=<AddBackward0>)\n",
      "tensor(2.5060, grad_fn=<AddBackward0>)\n",
      "tensor(2.5059, grad_fn=<AddBackward0>)\n",
      "tensor(2.5059, grad_fn=<AddBackward0>)\n",
      "tensor(2.5058, grad_fn=<AddBackward0>)\n",
      "tensor(2.5058, grad_fn=<AddBackward0>)\n",
      "tensor(2.5057, grad_fn=<AddBackward0>)\n",
      "tensor(2.5057, grad_fn=<AddBackward0>)\n",
      "tensor(2.5056, grad_fn=<AddBackward0>)\n",
      "tensor(2.5056, grad_fn=<AddBackward0>)\n",
      "tensor(2.5055, grad_fn=<AddBackward0>)\n",
      "tensor(2.5055, grad_fn=<AddBackward0>)\n",
      "tensor(2.5054, grad_fn=<AddBackward0>)\n",
      "tensor(2.5054, grad_fn=<AddBackward0>)\n",
      "tensor(2.5053, grad_fn=<AddBackward0>)\n",
      "tensor(2.5053, grad_fn=<AddBackward0>)\n",
      "tensor(2.5052, grad_fn=<AddBackward0>)\n",
      "tensor(2.5052, grad_fn=<AddBackward0>)\n",
      "tensor(2.5051, grad_fn=<AddBackward0>)\n",
      "tensor(2.5051, grad_fn=<AddBackward0>)\n",
      "tensor(2.5050, grad_fn=<AddBackward0>)\n",
      "tensor(2.5050, grad_fn=<AddBackward0>)\n",
      "tensor(2.5049, grad_fn=<AddBackward0>)\n",
      "tensor(2.5049, grad_fn=<AddBackward0>)\n",
      "tensor(2.5049, grad_fn=<AddBackward0>)\n",
      "tensor(2.5048, grad_fn=<AddBackward0>)\n",
      "tensor(2.5048, grad_fn=<AddBackward0>)\n",
      "tensor(2.5047, grad_fn=<AddBackward0>)\n",
      "tensor(2.5047, grad_fn=<AddBackward0>)\n",
      "tensor(2.5046, grad_fn=<AddBackward0>)\n",
      "tensor(2.5046, grad_fn=<AddBackward0>)\n",
      "tensor(2.5045, grad_fn=<AddBackward0>)\n",
      "tensor(2.5045, grad_fn=<AddBackward0>)\n",
      "tensor(2.5045, grad_fn=<AddBackward0>)\n",
      "tensor(2.5044, grad_fn=<AddBackward0>)\n",
      "tensor(2.5044, grad_fn=<AddBackward0>)\n",
      "tensor(2.5043, grad_fn=<AddBackward0>)\n",
      "tensor(2.5043, grad_fn=<AddBackward0>)\n",
      "tensor(2.5042, grad_fn=<AddBackward0>)\n",
      "tensor(2.5042, grad_fn=<AddBackward0>)\n",
      "tensor(2.5042, grad_fn=<AddBackward0>)\n",
      "tensor(2.5041, grad_fn=<AddBackward0>)\n",
      "tensor(2.5041, grad_fn=<AddBackward0>)\n",
      "tensor(2.5040, grad_fn=<AddBackward0>)\n",
      "tensor(2.5040, grad_fn=<AddBackward0>)\n",
      "tensor(2.5040, grad_fn=<AddBackward0>)\n",
      "tensor(2.5039, grad_fn=<AddBackward0>)\n",
      "tensor(2.5039, grad_fn=<AddBackward0>)\n",
      "tensor(2.5038, grad_fn=<AddBackward0>)\n",
      "tensor(2.5038, grad_fn=<AddBackward0>)\n",
      "tensor(2.5038, grad_fn=<AddBackward0>)\n",
      "tensor(2.5037, grad_fn=<AddBackward0>)\n",
      "tensor(2.5037, grad_fn=<AddBackward0>)\n",
      "tensor(2.5037, grad_fn=<AddBackward0>)\n",
      "tensor(2.5036, grad_fn=<AddBackward0>)\n",
      "tensor(2.5036, grad_fn=<AddBackward0>)\n",
      "tensor(2.5035, grad_fn=<AddBackward0>)\n",
      "tensor(2.5035, grad_fn=<AddBackward0>)\n",
      "tensor(2.5035, grad_fn=<AddBackward0>)\n",
      "tensor(2.5034, grad_fn=<AddBackward0>)\n",
      "tensor(2.5034, grad_fn=<AddBackward0>)\n",
      "tensor(2.5034, grad_fn=<AddBackward0>)\n",
      "tensor(2.5033, grad_fn=<AddBackward0>)\n",
      "tensor(2.5033, grad_fn=<AddBackward0>)\n",
      "tensor(2.5033, grad_fn=<AddBackward0>)\n",
      "tensor(2.5032, grad_fn=<AddBackward0>)\n",
      "tensor(2.5032, grad_fn=<AddBackward0>)\n",
      "tensor(2.5032, grad_fn=<AddBackward0>)\n",
      "tensor(2.5031, grad_fn=<AddBackward0>)\n",
      "tensor(2.5031, grad_fn=<AddBackward0>)\n",
      "tensor(2.5030, grad_fn=<AddBackward0>)\n",
      "tensor(2.5030, grad_fn=<AddBackward0>)\n",
      "tensor(2.5030, grad_fn=<AddBackward0>)\n",
      "tensor(2.5029, grad_fn=<AddBackward0>)\n",
      "tensor(2.5029, grad_fn=<AddBackward0>)\n",
      "tensor(2.5029, grad_fn=<AddBackward0>)\n",
      "tensor(2.5029, grad_fn=<AddBackward0>)\n",
      "tensor(2.5028, grad_fn=<AddBackward0>)\n",
      "tensor(2.5028, grad_fn=<AddBackward0>)\n",
      "tensor(2.5028, grad_fn=<AddBackward0>)\n",
      "tensor(2.5027, grad_fn=<AddBackward0>)\n",
      "tensor(2.5027, grad_fn=<AddBackward0>)\n",
      "tensor(2.5027, grad_fn=<AddBackward0>)\n",
      "tensor(2.5026, grad_fn=<AddBackward0>)\n",
      "tensor(2.5026, grad_fn=<AddBackward0>)\n",
      "tensor(2.5026, grad_fn=<AddBackward0>)\n",
      "tensor(2.5025, grad_fn=<AddBackward0>)\n",
      "tensor(2.5025, grad_fn=<AddBackward0>)\n",
      "tensor(2.5025, grad_fn=<AddBackward0>)\n",
      "tensor(2.5024, grad_fn=<AddBackward0>)\n",
      "tensor(2.5024, grad_fn=<AddBackward0>)\n",
      "tensor(2.5024, grad_fn=<AddBackward0>)\n",
      "tensor(2.5024, grad_fn=<AddBackward0>)\n",
      "tensor(2.5023, grad_fn=<AddBackward0>)\n",
      "tensor(2.5023, grad_fn=<AddBackward0>)\n",
      "tensor(2.5023, grad_fn=<AddBackward0>)\n",
      "tensor(2.5022, grad_fn=<AddBackward0>)\n",
      "tensor(2.5022, grad_fn=<AddBackward0>)\n",
      "tensor(2.5022, grad_fn=<AddBackward0>)\n",
      "tensor(2.5022, grad_fn=<AddBackward0>)\n",
      "tensor(2.5021, grad_fn=<AddBackward0>)\n",
      "tensor(2.5021, grad_fn=<AddBackward0>)\n",
      "tensor(2.5021, grad_fn=<AddBackward0>)\n",
      "tensor(2.5020, grad_fn=<AddBackward0>)\n",
      "tensor(2.5020, grad_fn=<AddBackward0>)\n",
      "tensor(2.5020, grad_fn=<AddBackward0>)\n",
      "tensor(2.5020, grad_fn=<AddBackward0>)\n",
      "tensor(2.5019, grad_fn=<AddBackward0>)\n",
      "tensor(2.5019, grad_fn=<AddBackward0>)\n",
      "tensor(2.5019, grad_fn=<AddBackward0>)\n",
      "tensor(2.5018, grad_fn=<AddBackward0>)\n",
      "tensor(2.5018, grad_fn=<AddBackward0>)\n",
      "tensor(2.5018, grad_fn=<AddBackward0>)\n",
      "tensor(2.5018, grad_fn=<AddBackward0>)\n",
      "tensor(2.5017, grad_fn=<AddBackward0>)\n",
      "tensor(2.5017, grad_fn=<AddBackward0>)\n",
      "tensor(2.5017, grad_fn=<AddBackward0>)\n",
      "tensor(2.5017, grad_fn=<AddBackward0>)\n",
      "tensor(2.5016, grad_fn=<AddBackward0>)\n",
      "tensor(2.5016, grad_fn=<AddBackward0>)\n",
      "tensor(2.5016, grad_fn=<AddBackward0>)\n",
      "tensor(2.5016, grad_fn=<AddBackward0>)\n",
      "tensor(2.5015, grad_fn=<AddBackward0>)\n",
      "tensor(2.5015, grad_fn=<AddBackward0>)\n",
      "tensor(2.5015, grad_fn=<AddBackward0>)\n",
      "tensor(2.5015, grad_fn=<AddBackward0>)\n",
      "tensor(2.5014, grad_fn=<AddBackward0>)\n",
      "tensor(2.5014, grad_fn=<AddBackward0>)\n",
      "tensor(2.5014, grad_fn=<AddBackward0>)\n",
      "tensor(2.5014, grad_fn=<AddBackward0>)\n",
      "tensor(2.5013, grad_fn=<AddBackward0>)\n",
      "tensor(2.5013, grad_fn=<AddBackward0>)\n",
      "tensor(2.5013, grad_fn=<AddBackward0>)\n",
      "tensor(2.5013, grad_fn=<AddBackward0>)\n",
      "tensor(2.5012, grad_fn=<AddBackward0>)\n",
      "tensor(2.5012, grad_fn=<AddBackward0>)\n",
      "tensor(2.5012, grad_fn=<AddBackward0>)\n",
      "tensor(2.5012, grad_fn=<AddBackward0>)\n",
      "tensor(2.5012, grad_fn=<AddBackward0>)\n",
      "tensor(2.5011, grad_fn=<AddBackward0>)\n",
      "tensor(2.5011, grad_fn=<AddBackward0>)\n",
      "tensor(2.5011, grad_fn=<AddBackward0>)\n",
      "tensor(2.5011, grad_fn=<AddBackward0>)\n",
      "tensor(2.5010, grad_fn=<AddBackward0>)\n",
      "tensor(2.5010, grad_fn=<AddBackward0>)\n",
      "tensor(2.5010, grad_fn=<AddBackward0>)\n",
      "tensor(2.5010, grad_fn=<AddBackward0>)\n",
      "tensor(2.5010, grad_fn=<AddBackward0>)\n",
      "tensor(2.5009, grad_fn=<AddBackward0>)\n",
      "tensor(2.5009, grad_fn=<AddBackward0>)\n",
      "tensor(2.5009, grad_fn=<AddBackward0>)\n",
      "tensor(2.5009, grad_fn=<AddBackward0>)\n",
      "tensor(2.5009, grad_fn=<AddBackward0>)\n",
      "tensor(2.5008, grad_fn=<AddBackward0>)\n",
      "tensor(2.5008, grad_fn=<AddBackward0>)\n",
      "tensor(2.5008, grad_fn=<AddBackward0>)\n",
      "tensor(2.5008, grad_fn=<AddBackward0>)\n",
      "tensor(2.5007, grad_fn=<AddBackward0>)\n",
      "tensor(2.5007, grad_fn=<AddBackward0>)\n",
      "tensor(2.5007, grad_fn=<AddBackward0>)\n",
      "tensor(2.5007, grad_fn=<AddBackward0>)\n",
      "tensor(2.5007, grad_fn=<AddBackward0>)\n",
      "tensor(2.5006, grad_fn=<AddBackward0>)\n",
      "tensor(2.5006, grad_fn=<AddBackward0>)\n",
      "tensor(2.5006, grad_fn=<AddBackward0>)\n",
      "tensor(2.5006, grad_fn=<AddBackward0>)\n",
      "tensor(2.5006, grad_fn=<AddBackward0>)\n",
      "tensor(2.5005, grad_fn=<AddBackward0>)\n",
      "tensor(2.5005, grad_fn=<AddBackward0>)\n",
      "tensor(2.5005, grad_fn=<AddBackward0>)\n",
      "tensor(2.5005, grad_fn=<AddBackward0>)\n",
      "tensor(2.5005, grad_fn=<AddBackward0>)\n",
      "tensor(2.5005, grad_fn=<AddBackward0>)\n",
      "tensor(2.5004, grad_fn=<AddBackward0>)\n",
      "tensor(2.5004, grad_fn=<AddBackward0>)\n",
      "tensor(2.5004, grad_fn=<AddBackward0>)\n",
      "tensor(2.5004, grad_fn=<AddBackward0>)\n",
      "tensor(2.5004, grad_fn=<AddBackward0>)\n",
      "tensor(2.5003, grad_fn=<AddBackward0>)\n",
      "tensor(2.5003, grad_fn=<AddBackward0>)\n",
      "tensor(2.5003, grad_fn=<AddBackward0>)\n",
      "tensor(2.5003, grad_fn=<AddBackward0>)\n",
      "tensor(2.5003, grad_fn=<AddBackward0>)\n",
      "tensor(2.5002, grad_fn=<AddBackward0>)\n",
      "tensor(2.5002, grad_fn=<AddBackward0>)\n",
      "tensor(2.5002, grad_fn=<AddBackward0>)\n",
      "tensor(2.5002, grad_fn=<AddBackward0>)\n",
      "tensor(2.5002, grad_fn=<AddBackward0>)\n",
      "tensor(2.5002, grad_fn=<AddBackward0>)\n",
      "tensor(2.5001, grad_fn=<AddBackward0>)\n",
      "tensor(2.5001, grad_fn=<AddBackward0>)\n",
      "tensor(2.5001, grad_fn=<AddBackward0>)\n",
      "tensor(2.5001, grad_fn=<AddBackward0>)\n",
      "tensor(2.5001, grad_fn=<AddBackward0>)\n",
      "tensor(2.5001, grad_fn=<AddBackward0>)\n",
      "tensor(2.5000, grad_fn=<AddBackward0>)\n",
      "tensor(2.5000, grad_fn=<AddBackward0>)\n",
      "tensor(2.5000, grad_fn=<AddBackward0>)\n",
      "tensor(2.5000, grad_fn=<AddBackward0>)\n",
      "tensor(2.5000, grad_fn=<AddBackward0>)\n",
      "tensor(2.5000, grad_fn=<AddBackward0>)\n",
      "tensor(2.4999, grad_fn=<AddBackward0>)\n",
      "tensor(2.4999, grad_fn=<AddBackward0>)\n",
      "tensor(2.4999, grad_fn=<AddBackward0>)\n",
      "tensor(2.4999, grad_fn=<AddBackward0>)\n",
      "tensor(2.4999, grad_fn=<AddBackward0>)\n",
      "tensor(2.4999, grad_fn=<AddBackward0>)\n",
      "tensor(2.4998, grad_fn=<AddBackward0>)\n",
      "tensor(2.4998, grad_fn=<AddBackward0>)\n",
      "tensor(2.4998, grad_fn=<AddBackward0>)\n",
      "tensor(2.4998, grad_fn=<AddBackward0>)\n",
      "tensor(2.4998, grad_fn=<AddBackward0>)\n",
      "tensor(2.4998, grad_fn=<AddBackward0>)\n",
      "tensor(2.4998, grad_fn=<AddBackward0>)\n",
      "tensor(2.4997, grad_fn=<AddBackward0>)\n",
      "tensor(2.4997, grad_fn=<AddBackward0>)\n",
      "tensor(2.4997, grad_fn=<AddBackward0>)\n",
      "tensor(2.4997, grad_fn=<AddBackward0>)\n",
      "tensor(2.4997, grad_fn=<AddBackward0>)\n",
      "tensor(2.4997, grad_fn=<AddBackward0>)\n",
      "tensor(2.4996, grad_fn=<AddBackward0>)\n",
      "tensor(2.4996, grad_fn=<AddBackward0>)\n",
      "tensor(2.4996, grad_fn=<AddBackward0>)\n",
      "tensor(2.4996, grad_fn=<AddBackward0>)\n",
      "tensor(2.4996, grad_fn=<AddBackward0>)\n",
      "tensor(2.4996, grad_fn=<AddBackward0>)\n",
      "tensor(2.4996, grad_fn=<AddBackward0>)\n",
      "tensor(2.4995, grad_fn=<AddBackward0>)\n",
      "tensor(2.4995, grad_fn=<AddBackward0>)\n",
      "tensor(2.4995, grad_fn=<AddBackward0>)\n",
      "tensor(2.4995, grad_fn=<AddBackward0>)\n",
      "tensor(2.4995, grad_fn=<AddBackward0>)\n",
      "tensor(2.4995, grad_fn=<AddBackward0>)\n",
      "tensor(2.4995, grad_fn=<AddBackward0>)\n",
      "tensor(2.4994, grad_fn=<AddBackward0>)\n",
      "tensor(2.4994, grad_fn=<AddBackward0>)\n",
      "tensor(2.4994, grad_fn=<AddBackward0>)\n",
      "tensor(2.4994, grad_fn=<AddBackward0>)\n",
      "tensor(2.4994, grad_fn=<AddBackward0>)\n",
      "tensor(2.4994, grad_fn=<AddBackward0>)\n",
      "tensor(2.4994, grad_fn=<AddBackward0>)\n",
      "tensor(2.4994, grad_fn=<AddBackward0>)\n",
      "tensor(2.4993, grad_fn=<AddBackward0>)\n",
      "tensor(2.4993, grad_fn=<AddBackward0>)\n",
      "tensor(2.4993, grad_fn=<AddBackward0>)\n",
      "tensor(2.4993, grad_fn=<AddBackward0>)\n",
      "tensor(2.4993, grad_fn=<AddBackward0>)\n",
      "tensor(2.4993, grad_fn=<AddBackward0>)\n",
      "tensor(2.4993, grad_fn=<AddBackward0>)\n",
      "tensor(2.4993, grad_fn=<AddBackward0>)\n",
      "tensor(2.4992, grad_fn=<AddBackward0>)\n",
      "tensor(2.4992, grad_fn=<AddBackward0>)\n",
      "tensor(2.4992, grad_fn=<AddBackward0>)\n",
      "tensor(2.4992, grad_fn=<AddBackward0>)\n",
      "tensor(2.4992, grad_fn=<AddBackward0>)\n",
      "tensor(2.4992, grad_fn=<AddBackward0>)\n",
      "tensor(2.4992, grad_fn=<AddBackward0>)\n",
      "tensor(2.4992, grad_fn=<AddBackward0>)\n",
      "tensor(2.4991, grad_fn=<AddBackward0>)\n",
      "tensor(2.4991, grad_fn=<AddBackward0>)\n",
      "tensor(2.4991, grad_fn=<AddBackward0>)\n",
      "tensor(2.4991, grad_fn=<AddBackward0>)\n",
      "tensor(2.4991, grad_fn=<AddBackward0>)\n",
      "tensor(2.4991, grad_fn=<AddBackward0>)\n",
      "tensor(2.4991, grad_fn=<AddBackward0>)\n",
      "tensor(2.4991, grad_fn=<AddBackward0>)\n",
      "tensor(2.4990, grad_fn=<AddBackward0>)\n",
      "tensor(2.4990, grad_fn=<AddBackward0>)\n",
      "tensor(2.4990, grad_fn=<AddBackward0>)\n",
      "tensor(2.4990, grad_fn=<AddBackward0>)\n",
      "tensor(2.4990, grad_fn=<AddBackward0>)\n",
      "tensor(2.4990, grad_fn=<AddBackward0>)\n",
      "tensor(2.4990, grad_fn=<AddBackward0>)\n",
      "tensor(2.4990, grad_fn=<AddBackward0>)\n",
      "tensor(2.4989, grad_fn=<AddBackward0>)\n",
      "tensor(2.4989, grad_fn=<AddBackward0>)\n",
      "tensor(2.4989, grad_fn=<AddBackward0>)\n",
      "tensor(2.4989, grad_fn=<AddBackward0>)\n",
      "tensor(2.4989, grad_fn=<AddBackward0>)\n",
      "tensor(2.4989, grad_fn=<AddBackward0>)\n",
      "tensor(2.4989, grad_fn=<AddBackward0>)\n",
      "tensor(2.4989, grad_fn=<AddBackward0>)\n",
      "tensor(2.4989, grad_fn=<AddBackward0>)\n",
      "tensor(2.4989, grad_fn=<AddBackward0>)\n",
      "tensor(2.4988, grad_fn=<AddBackward0>)\n",
      "tensor(2.4988, grad_fn=<AddBackward0>)\n",
      "tensor(2.4988, grad_fn=<AddBackward0>)\n",
      "tensor(2.4988, grad_fn=<AddBackward0>)\n",
      "tensor(2.4988, grad_fn=<AddBackward0>)\n",
      "tensor(2.4988, grad_fn=<AddBackward0>)\n",
      "tensor(2.4988, grad_fn=<AddBackward0>)\n",
      "tensor(2.4988, grad_fn=<AddBackward0>)\n",
      "tensor(2.4988, grad_fn=<AddBackward0>)\n",
      "tensor(2.4987, grad_fn=<AddBackward0>)\n",
      "tensor(2.4987, grad_fn=<AddBackward0>)\n",
      "tensor(2.4987, grad_fn=<AddBackward0>)\n",
      "tensor(2.4987, grad_fn=<AddBackward0>)\n",
      "tensor(2.4987, grad_fn=<AddBackward0>)\n",
      "tensor(2.4987, grad_fn=<AddBackward0>)\n",
      "tensor(2.4987, grad_fn=<AddBackward0>)\n",
      "tensor(2.4987, grad_fn=<AddBackward0>)\n",
      "tensor(2.4987, grad_fn=<AddBackward0>)\n",
      "tensor(2.4987, grad_fn=<AddBackward0>)\n",
      "tensor(2.4986, grad_fn=<AddBackward0>)\n",
      "tensor(2.4986, grad_fn=<AddBackward0>)\n",
      "tensor(2.4986, grad_fn=<AddBackward0>)\n",
      "tensor(2.4986, grad_fn=<AddBackward0>)\n",
      "tensor(2.4986, grad_fn=<AddBackward0>)\n",
      "tensor(2.4986, grad_fn=<AddBackward0>)\n",
      "tensor(2.4986, grad_fn=<AddBackward0>)\n",
      "tensor(2.4986, grad_fn=<AddBackward0>)\n",
      "tensor(2.4986, grad_fn=<AddBackward0>)\n",
      "tensor(2.4986, grad_fn=<AddBackward0>)\n",
      "tensor(2.4985, grad_fn=<AddBackward0>)\n",
      "tensor(2.4985, grad_fn=<AddBackward0>)\n",
      "tensor(2.4985, grad_fn=<AddBackward0>)\n",
      "tensor(2.4985, grad_fn=<AddBackward0>)\n",
      "tensor(2.4985, grad_fn=<AddBackward0>)\n",
      "tensor(2.4985, grad_fn=<AddBackward0>)\n",
      "tensor(2.4985, grad_fn=<AddBackward0>)\n",
      "tensor(2.4985, grad_fn=<AddBackward0>)\n",
      "tensor(2.4985, grad_fn=<AddBackward0>)\n",
      "tensor(2.4985, grad_fn=<AddBackward0>)\n",
      "tensor(2.4985, grad_fn=<AddBackward0>)\n",
      "tensor(2.4984, grad_fn=<AddBackward0>)\n",
      "tensor(2.4984, grad_fn=<AddBackward0>)\n",
      "tensor(2.4984, grad_fn=<AddBackward0>)\n",
      "tensor(2.4984, grad_fn=<AddBackward0>)\n",
      "tensor(2.4984, grad_fn=<AddBackward0>)\n",
      "tensor(2.4984, grad_fn=<AddBackward0>)\n",
      "tensor(2.4984, grad_fn=<AddBackward0>)\n",
      "tensor(2.4984, grad_fn=<AddBackward0>)\n",
      "tensor(2.4984, grad_fn=<AddBackward0>)\n",
      "tensor(2.4984, grad_fn=<AddBackward0>)\n",
      "tensor(2.4984, grad_fn=<AddBackward0>)\n",
      "tensor(2.4984, grad_fn=<AddBackward0>)\n",
      "tensor(2.4983, grad_fn=<AddBackward0>)\n",
      "tensor(2.4983, grad_fn=<AddBackward0>)\n",
      "tensor(2.4983, grad_fn=<AddBackward0>)\n",
      "tensor(2.4983, grad_fn=<AddBackward0>)\n",
      "tensor(2.4983, grad_fn=<AddBackward0>)\n",
      "tensor(2.4983, grad_fn=<AddBackward0>)\n",
      "tensor(2.4983, grad_fn=<AddBackward0>)\n",
      "tensor(2.4983, grad_fn=<AddBackward0>)\n",
      "tensor(2.4983, grad_fn=<AddBackward0>)\n",
      "tensor(2.4983, grad_fn=<AddBackward0>)\n",
      "tensor(2.4983, grad_fn=<AddBackward0>)\n",
      "tensor(2.4983, grad_fn=<AddBackward0>)\n",
      "tensor(2.4982, grad_fn=<AddBackward0>)\n",
      "tensor(2.4982, grad_fn=<AddBackward0>)\n",
      "tensor(2.4982, grad_fn=<AddBackward0>)\n",
      "tensor(2.4982, grad_fn=<AddBackward0>)\n",
      "tensor(2.4982, grad_fn=<AddBackward0>)\n",
      "tensor(2.4982, grad_fn=<AddBackward0>)\n",
      "tensor(2.4982, grad_fn=<AddBackward0>)\n",
      "tensor(2.4982, grad_fn=<AddBackward0>)\n",
      "tensor(2.4982, grad_fn=<AddBackward0>)\n",
      "tensor(2.4982, grad_fn=<AddBackward0>)\n",
      "tensor(2.4982, grad_fn=<AddBackward0>)\n",
      "tensor(2.4982, grad_fn=<AddBackward0>)\n",
      "tensor(2.4982, grad_fn=<AddBackward0>)\n",
      "tensor(2.4981, grad_fn=<AddBackward0>)\n",
      "tensor(2.4981, grad_fn=<AddBackward0>)\n",
      "tensor(2.4981, grad_fn=<AddBackward0>)\n",
      "tensor(2.4981, grad_fn=<AddBackward0>)\n",
      "tensor(2.4981, grad_fn=<AddBackward0>)\n",
      "tensor(2.4981, grad_fn=<AddBackward0>)\n",
      "tensor(2.4981, grad_fn=<AddBackward0>)\n",
      "tensor(2.4981, grad_fn=<AddBackward0>)\n",
      "tensor(2.4981, grad_fn=<AddBackward0>)\n",
      "tensor(2.4981, grad_fn=<AddBackward0>)\n",
      "tensor(2.4981, grad_fn=<AddBackward0>)\n",
      "tensor(2.4981, grad_fn=<AddBackward0>)\n",
      "tensor(2.4981, grad_fn=<AddBackward0>)\n",
      "tensor(2.4980, grad_fn=<AddBackward0>)\n",
      "tensor(2.4980, grad_fn=<AddBackward0>)\n",
      "tensor(2.4980, grad_fn=<AddBackward0>)\n",
      "tensor(2.4980, grad_fn=<AddBackward0>)\n",
      "tensor(2.4980, grad_fn=<AddBackward0>)\n",
      "tensor(2.4980, grad_fn=<AddBackward0>)\n",
      "tensor(2.4980, grad_fn=<AddBackward0>)\n",
      "tensor(2.4980, grad_fn=<AddBackward0>)\n",
      "tensor(2.4980, grad_fn=<AddBackward0>)\n",
      "tensor(2.4980, grad_fn=<AddBackward0>)\n",
      "tensor(2.4980, grad_fn=<AddBackward0>)\n",
      "tensor(2.4980, grad_fn=<AddBackward0>)\n",
      "tensor(2.4980, grad_fn=<AddBackward0>)\n",
      "tensor(2.4980, grad_fn=<AddBackward0>)\n",
      "tensor(2.4980, grad_fn=<AddBackward0>)\n",
      "tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4978, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4977, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4976, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4975, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4974, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4973, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4972, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4971, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "tensor(2.4970, grad_fn=<AddBackward0>)\n",
      "Reg: 0.0001, Dev Loss: 2.4593379497528076\n"
     ]
    }
   ],
   "source": [
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647 + 1)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True) * 0.01 # tuning\n",
    "reg_strengths = [0.0001] \n",
    "results = []\n",
    "for reg_lambda in reg_strengths:\n",
    "    W = torch.randn((27, 27), generator=g, requires_grad=True)  # Reinitialize W\n",
    "\n",
    "    for k in range(1000):  # Training loop\n",
    "        xenc = F.one_hot(X_train, num_classes=27).float().max(dim=1)[0]\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "        loss = -probs[torch.arange(X_train.shape[0]), y_train].log().mean() + reg_lambda * (W**2).sum()\n",
    "        print(loss)\n",
    "\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        W.data += -6 * W.grad\n",
    "\n",
    "    # Compute final loss on dev set\n",
    "    xenc_dev = F.one_hot(X_dev, num_classes=27).float().max(dim=1)[0]\n",
    "    logits_dev = xenc_dev @ W\n",
    "    counts_dev = logits_dev.exp()\n",
    "    probs_dev = counts_dev / counts_dev.sum(1, keepdims=True)\n",
    "    dev_loss = -probs_dev[torch.arange(X_dev.shape[0]), y_dev].log().mean().item()\n",
    "\n",
    "    results.append((reg_lambda, dev_loss))\n",
    "    print(f\"Reg: {reg_lambda}, Dev Loss: {dev_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "b110deee-3b91-4051-8e8c-35a8448fbca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4657070636749268\n"
     ]
    }
   ],
   "source": [
    "xenc = F.one_hot(X_test, num_classes=27).float().max(dim=1)[0] # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(X_test.shape[0]), y_test].log().mean()\n",
    "test_error = loss.item()\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f82c1-1de2-4056-bd06-5613ecfca88c",
   "metadata": {},
   "source": [
    "**E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "id": "f456b707-3e58-4ba3-8a59-8dd328e5c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "for w in words: \n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1, ix2))\n",
    "        ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "5eea2812-c245-4c71-9b91-5350acbd53c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([237192, 2]), torch.Size([237192]))"
      ]
     },
     "execution_count": 748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "559b30c4-0afc-4f29-970d-a1464a3984dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27, 27), generator=g, requires_grad=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "0cc45e28-e401-4739-af21-1bf437da808e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1426122188568115\n",
      "2.142608404159546\n",
      "2.142604351043701\n",
      "2.1426005363464355\n",
      "2.142596483230591\n",
      "2.142592668533325\n",
      "2.1425886154174805\n",
      "2.1425845623016357\n",
      "2.14258074760437\n",
      "2.1425766944885254\n",
      "2.1425728797912598\n",
      "2.142568826675415\n",
      "2.1425647735595703\n",
      "2.1425609588623047\n",
      "2.142557144165039\n",
      "2.1425530910491943\n",
      "2.1425492763519287\n",
      "2.142545223236084\n",
      "2.1425411701202393\n",
      "2.1425375938415527\n",
      "2.142533540725708\n",
      "2.1425294876098633\n",
      "2.1425256729125977\n",
      "2.142521619796753\n",
      "2.1425178050994873\n",
      "2.1425139904022217\n",
      "2.142510175704956\n",
      "2.1425061225891113\n",
      "2.1425020694732666\n",
      "2.14249849319458\n",
      "2.1424942016601562\n",
      "2.1424903869628906\n",
      "2.142486572265625\n",
      "2.1424827575683594\n",
      "2.1424784660339355\n",
      "2.142474889755249\n",
      "2.1424708366394043\n",
      "2.1424670219421387\n",
      "2.142463207244873\n",
      "2.1424591541290283\n",
      "2.1424553394317627\n",
      "2.142451524734497\n",
      "2.1424477100372314\n",
      "2.1424436569213867\n",
      "2.142439603805542\n",
      "2.1424360275268555\n",
      "2.1424319744110107\n",
      "2.142428159713745\n",
      "2.1424243450164795\n",
      "2.142420530319214\n",
      "2.142416477203369\n",
      "2.1424126625061035\n",
      "2.142408609390259\n",
      "2.142404794692993\n",
      "2.1424009799957275\n",
      "2.142397403717041\n",
      "2.1423933506011963\n",
      "2.1423892974853516\n",
      "2.142385482788086\n",
      "2.1423816680908203\n",
      "2.1423778533935547\n",
      "2.142374038696289\n",
      "2.1423699855804443\n",
      "2.1423661708831787\n",
      "2.142362594604492\n",
      "2.1423587799072266\n",
      "2.142354726791382\n",
      "2.142350673675537\n",
      "2.1423468589782715\n",
      "2.142343044281006\n",
      "2.1423392295837402\n",
      "2.1423354148864746\n",
      "2.14233136177063\n",
      "2.1423277854919434\n",
      "2.1423237323760986\n",
      "2.142320156097412\n",
      "2.1423161029815674\n",
      "2.1423122882843018\n",
      "2.1423087120056152\n",
      "2.1423048973083496\n",
      "2.142301082611084\n",
      "2.1422972679138184\n",
      "2.1422932147979736\n",
      "2.142289161682129\n",
      "2.1422855854034424\n",
      "2.1422817707061768\n",
      "2.142277956008911\n",
      "2.1422741413116455\n",
      "2.14227032661438\n",
      "2.142266273498535\n",
      "2.1422626972198486\n",
      "2.142258644104004\n",
      "2.1422550678253174\n",
      "2.1422512531280518\n",
      "2.142247438430786\n",
      "2.1422436237335205\n",
      "2.142239809036255\n",
      "2.1422359943389893\n",
      "2.1422324180603027\n",
      "2.142228364944458\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for k in range(100):\n",
    "    logits = W[xs[:, 0], xs[:, 1]]  # Replace one-hot encoding\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(xs.shape[0]), ys].log().mean()\n",
    "\n",
    "    print(loss.item())\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -30 * W.grad  # Adjusted learning rate for stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "a95e93c2-d5a5-4f47-96af-ee4a4d377837",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = 2.142228364944458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "13a782e2-1d2a-4155-88c4-946227a052ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1391844749450684\n"
     ]
    }
   ],
   "source": [
    "logits = W[X_dev[:, 0], X_dev[:, 1]]  # Replace one-hot encoding\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(X_dev.shape[0]), y_dev].log().mean()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c01d02-9fad-4dde-bcee-d39dd4e70548",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "id": "2c6f07ab-f94d-466d-92b2-6e1e06fe6c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odeperry.\n",
      "zablalik.\n",
      "rossa.\n",
      "osim.\n",
      "tereanja.\n"
     ]
    }
   ],
   "source": [
    "# Finally, sample from the neural net model\n",
    "g = torch.Generator().manual_seed(2147483647+24)\n",
    "\n",
    "for i in range(5):  # Generate 5 words\n",
    "    out = []\n",
    "    ix1, ix2 = 0, 0  # Start with two dots (start tokens)\n",
    "\n",
    "    while True:\n",
    "        # Use indexing instead of one-hot encoding\n",
    "        logits = W[ix1, ix2]  # Directly fetch logits from W\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum()\n",
    "\n",
    "        # Sample the next character\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        out.append(itos[ix])  # Convert index to character\n",
    "\n",
    "        if ix == 0:  # Stop if end token is reached\n",
    "            break\n",
    "\n",
    "        # Shift indices: new pair is (previous ix2, new ix)\n",
    "        ix1, ix2 = ix2, ix\n",
    "\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89465e-d1a4-422f-a517-d90c9dc9434a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
